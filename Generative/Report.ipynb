{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0edd7a90",
   "metadata": {},
   "source": [
    "## Report Notebook\n",
    "\n",
    "This is just a notebook to compare the results of the generative models on several datasets, saving the results on wandb as tables\n",
    "\n",
    "Models to compare:\n",
    "\n",
    "VAE    <br>\n",
    "VQVAE  <br>\n",
    "VQVAE2 <br>\n",
    "\n",
    "GAN    <br>\n",
    "WGAN   <br>\n",
    "PCGAN  <br>\n",
    "\n",
    "VAEGAN [ ] <br> \n",
    "VQGAN  [ ] <br> \n",
    "\n",
    "NVAE   [ ] <br> \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cb93c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "import numpy as np\n",
    "import os\n",
    "from os.path import join as opj\n",
    "from classes.VAE import VAE\n",
    "from classes.VQVAE import VQVAE, VQVAE2\n",
    "from classes.GAN import GAN\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.data import AUTOTUNE\n",
    "from imutils import paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df622998",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmatteoferrante\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "wandb.login()\n",
    "\n",
    "BS=128\n",
    "\n",
    "#set the second GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "print(os.environ.get(\"CUDA_VISIBLE_DEVICES\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e53e60e",
   "metadata": {},
   "source": [
    "### Models path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b54831fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_path=\"models/vae_celeba\"\n",
    "vqvae_path=\"models/vqvae_celeba\"\n",
    "vqvae2_path=\"models/vqvae2_celeba\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3495a374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vae ['vae_model.png', 'encoder_weights.h5', 'decoder_weights.h5']\n",
      "\n",
      " vqvae ['vqvae_model.png', 'vqvae_encoder_model.png', 'vqvae_decoder_model.png', 'vq_vae_encoder.h5', 'vq_vae_decoder.h5', 'vq_vae_embeddings.npy']\n",
      "\n",
      " vqvae2 ['vqvae2_encoder_bottom.png', 'vqvae2_encoder_top.png', 'vqvae2_decoder.png', 'vqvae2_conditional_bottom.png', 'vqvae2_model.png', 'vq_vae2_encoder_b.h5', 'vq_vae2_encoder_t.h5', 'vq_vae2_encoder_conditional_bottom.h5', 'vq_vae2_decoder.h5']\n"
     ]
    }
   ],
   "source": [
    "print(f\"vae {os.listdir(vae_path)}\")\n",
    "print(f\"\\n vqvae {os.listdir(vqvae_path)}\")\n",
    "print(f\"\\n vqvae2 {os.listdir(vqvae2_path)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd735b5",
   "metadata": {},
   "source": [
    "# CelebA comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a5686ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#images_dir=r\"C:\\Users\\matte\\Dataset\\tor_vergata\\Dataset\\Img\\img_align_celeba\" #local\n",
    "images_dir=r\"/home/matteo/NeuroGEN/Dataset/Img/img_align_celeba\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cedb019",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(imagePath):\n",
    "    # read the image from disk, decode it, resize it, and scale the\n",
    "    # pixels intensities to the range [0, 1]\n",
    "    image = tf.io.read_file(imagePath)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, (128, 128)) / 255.0\n",
    "\n",
    "    #eventually load other information like attributes here\n",
    "    \n",
    "    # return the image and the extra info\n",
    "    \n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256b4d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading image paths...\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading image paths...\")\n",
    "imagePaths = list(paths.list_images(images_dir))\n",
    "\n",
    "\n",
    "train_len=int(0.8*len(imagePaths))\n",
    "val_len=int(0.1*len(imagePaths))\n",
    "test_len=int(0.1*len(imagePaths))\n",
    "\n",
    "train_imgs=imagePaths[:train_len]                                #      80% for training\n",
    "val_imgs=imagePaths[train_len:train_len+val_len]                 #      10% for validation\n",
    "test_imgs=imagePaths[train_len+val_len:]                         #      10% for testing\n",
    "\n",
    "print(f\"[TRAINING]\\t {len(train_imgs)}\\n[VALIDATION]\\t {len(val_imgs)}\\n[TEST]\\t\\t {len(test_imgs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269fefb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAINING \n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_imgs)\n",
    "train_dataset = (train_dataset\n",
    "    .map(load_images, num_parallel_calls=AUTOTUNE)\n",
    "    .cache()\n",
    "    .repeat()\n",
    "    .batch(BS)\n",
    "    .shuffle(1024)\n",
    "    .prefetch(AUTOTUNE)\n",
    ")\n",
    "\n",
    "ts=len(train_imgs)//BS\n",
    "\n",
    "##VALIDATION\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(val_imgs)\n",
    "val_dataset = (val_dataset\n",
    "    .map(load_images, num_parallel_calls=AUTOTUNE)\n",
    "    .cache()\n",
    "    .repeat()\n",
    "    .shuffle(1024)\n",
    "    .batch(BS)\n",
    "    .prefetch(AUTOTUNE)\n",
    ")\n",
    "\n",
    "vs=len(val_imgs)//BS\n",
    "\n",
    "## TEST\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(test_imgs)\n",
    "test_dataset = (test_dataset\n",
    "    .map(load_images, num_parallel_calls=AUTOTUNE)\n",
    "    .cache()\n",
    "    .shuffle(1024)\n",
    "    .batch(BS)\n",
    "    .prefetch(AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbad68e-bac6-4184-a896-e548163753b5",
   "metadata": {},
   "source": [
    "## VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c924290",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_architecture=[(0,64),(0,128),(0,256),(0,384),(0,512)]\n",
    "decoder_architecture=[(0,512),(0,384),(0,256),(0,128),(0,64)]\n",
    "\n",
    "vae=VAE((128,128,3),\n",
    "      latent_dim=512,\n",
    "      encoder_architecture=encoder_architecture,\n",
    "      decoder_architecture=decoder_architecture,\n",
    "      output_channels=3)\n",
    "\n",
    "vae.encoder.load_weights(opj(vae_path,\"encoder_weights.h5\"))\n",
    "vae.decoder.load_weights(opj(vae_path,\"decoder_weights.h5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8950819-0471-44b8-91a1-fb41ed4cb043",
   "metadata": {},
   "source": [
    "## VQVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4034392f-d3e5-4aff-9af6-8f2fd37af022",
   "metadata": {},
   "outputs": [],
   "source": [
    "## re train the model and save the weights!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7314bfdf-90b0-4ffb-893f-62db0b50b344",
   "metadata": {},
   "source": [
    "## VQVAE2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821e0716-70a6-431d-b7f4-91c08ab8d1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape=(128,128,3)\n",
    "latent_dim=256\n",
    "num_embeddings=1024\n",
    "\n",
    "vqvae2=VQVAE2(input_shape,latent_dim=latent_dim,num_embeddings=num_embeddings,train_variance=4,n_res_channel=latent_dim,channels=128)\n",
    "vqvae2.encoder_b.load_weights(opj(vqvae2_path,\"vq_vae2_encoder_b.h5\"))\n",
    "vqvae2.encoder_t.load_weights(opj(vqvae2_path,\"vq_vae2_encoder_t.h5\"))\n",
    "vqvae2.conditional_bottom.load_weights(opj(vqvae2_path,'vq_vae2_encoder_conditional_bottom.h5'))\n",
    "vqvae2.decoder.load_weights(opj(vqvae2_path,'vq_vae2_decoder.h5'))\n",
    "\n",
    "## load also the codebooks!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f711c112-2fb5-4ce7-9cb5-f8735a3bf897",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=test_dataset.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a0445c-69f5-48f3-b529-8c361da1c337",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=vqvae2.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d786a26-5234-4263-9193-4946756cd146",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4be0ce-29f9-4155-8420-8bb6e66d92df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
