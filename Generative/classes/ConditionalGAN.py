import tqdm
import wandb
from numpy.random import randint

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers import *
from tensorflow.keras.models import Model, Sequential
import numpy as np
from tensorflow.keras.optimizers import RMSprop

from .GAN import ClipConstraint


class cGAN(keras.Model):

    """
    class for conditional Generative extending keras.Model

    The idea of conditional GANs is to pass a condition to the generator it has to learn p(X|C) where is X is the latent space
    to generate input of specific cases.
    conditions are passed as embedding both to the generator and the discriminator to introduce this information and the labels
    are generated in two step as in usual Generative cases:

    in the first step we train the discriminator with both real and fakes images generated by the generator.

    So we feed our discriminator with r (real images) with real conditions and f (fake images with random conditions)
    and labels 0 for fake and 1 for real

    In the second step we will train the generator using the Generative approach generating fake images with random conditions
    with measleading labels (1 even if are fake images). In this step we update just the weights of the generator

    """
    def __init__(self, latent_dim,dims=(28,28,1),n_classes=10,emb_dim=50):

        """

        :param latent_dim: int, dimension of the latent space
        :param dims: tuple, shape of the images
        :param n_classes: int, number of classes
        :param emb_dim: int, dimension of the embedding space
        """
        super().__init__()
        self.discriminator = self.build_discriminator(dims,n_classes,emb_dim)
        self.generator = self.build_generator(latent_dim,n_classes,emb_dim)
        self.gan=self.build_gan()
        self.latent_dim = latent_dim
        self.n_classes=n_classes
        self.emb_dim=emb_dim


    def compile(self, d_optimizer=tf.keras.optimizers.Adam(3e-4), g_optimizer=tf.keras.optimizers.Adam(3e-4), loss_fn=tf.keras.losses.BinaryCrossentropy(from_logits=True),d_accuracy=tf.keras.metrics.Accuracy()):
        """
        method to compile all modules of this model

        :param d_optimizer: optimizer for discriminator
        :param g_optimizer: optimizer for generator
        :param loss_fn:  loss function to use for discriminator and gan
        :param d_accuracy: metric to measure discriminator performances
        """

        super(cGAN, self).compile(run_eagerly=True)
        self.d_optimizer = d_optimizer
        self.g_optimizer = g_optimizer
        self.loss_fn = loss_fn
        self.d_accuracy=d_accuracy

    def call(self, inputs, training=None, mask=None):
        return self.gan(inputs)



    def train_step(self, data):
        """
        Training step of .fit method.
        It trains the discriminator using a concatenation of real and fake images labelled as 0 and 1 respectively.
        images are coupled with the conditions that are mapped to an embedding space in order to reproduce a specific label case

        conditions (that are basically labels ie 0,1,2..9 in the mnist dataset) are passed both to a generator and a discriminator.
        The labels for the discriminator are generated as in the uncodinationed Generative cases.

        In a second step it train the gan (generator->discriminator) with frozen discriminator weights and images
        generated from noise from generator, using misleading labels (images are fake but labels are 0, to let the
        generator learn how to foolish the discriminator

        :param data: data used for training (i.e the real images)
        :return: a dict of metrics including g_loss, d_loss and d_accuracy
        """

        x,conditions=data

        conditions=tf.cast(conditions,dtype=tf.int32)  #just to be coherent with random_conditions

        batch_size = tf.shape(x)[0]

        #fake 1, real 0


        ## step 1 train the discriminator with real and fake imgs

        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))


        # Decode them to fake images
        random_conditions = tf.random.uniform(shape=[batch_size,],minval=0,maxval=self.n_classes,dtype=tf.int32)
        generated_images = self.generator([random_latent_vectors,random_conditions])

        # Combine them with real images

        combined_images = tf.concat([generated_images, x], axis=0)
        combined_conditions=tf.concat([random_conditions,conditions],axis=0)

        # Assemble labels discriminating real from fake images
        labels = tf.concat(
            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0
        )

        # Add random noise to the labels - important trick!
        labels += 0.05 * tf.random.uniform(labels.shape)




        with tf.GradientTape() as tape:

            y_pred=self.discriminator([combined_images,combined_conditions])

            d_loss=self.loss_fn(labels,y_pred)
            d_acc=self.d_accuracy(tf.round(labels),tf.round(y_pred))

        grads=tape.gradient(d_loss,self.discriminator.trainable_weights)
        self.d_optimizer.apply_gradients(zip(grads,self.discriminator.trainable_weights))


        ## step 2. try to foolish the discriminator with fake images and backpropagate to generator

        random_latent_vectors=tf.random.normal(shape=(batch_size,self.latent_dim))
        random_conditions = randint(0, self.n_classes, batch_size)  #generate conditions

        misleading_labels=tf.zeros((batch_size,1))

        with tf.GradientTape() as tape:

            y_pred=self.discriminator([self.generator([random_latent_vectors,random_conditions]),random_conditions])
            g_loss=self.loss_fn(misleading_labels,y_pred)

        grads=tape.gradient(g_loss,self.generator.trainable_weights)
        self.g_optimizer.apply_gradients(zip(grads,self.generator.trainable_weights))


        return {"d_loss":d_loss,"g_loss":g_loss, "d_acc":d_acc}

    def build_discriminator(self,dims,n_classes,emb_dim):
        """

        :param dims: dimension of the input images
        :param n_classes: number of possible conditions
        :param emb_dim: dimension of the embedding space
        :return: keras model of discriminator
        """

        ## label input

        in_label=Input(shape=(1,))                  #input for condition the class
        li=Embedding(n_classes,emb_dim)(in_label)              #li stands for label input
        li=Dense(np.prod(dims))(li)
        li = Reshape((dims[0], dims[1], 1))(li)        #produce image compatible shapes

        #image input
        in_image = Input(shape=dims)

        merge = Concatenate()([in_image, li])           #concatenate into a multichannel image

        ## discriminator

        x=Conv2D(128, (3, 3), strides=(2, 2), padding="same")(merge)
        x=LeakyReLU(alpha=0.2)(x)
        x=Conv2D(128, (3, 3), strides=(2, 2), padding="same")(x)
        x=LeakyReLU(alpha=0.2)(x)
        x=GlobalMaxPooling2D()(x)
        x=Dense(1)(x)

        model = Model([in_image, in_label], x,name="discriminator")

        return model


    def build_generator(self,latent_dim,n_classes,emb_dim):

        """
        :param dims: dimension of the input images
        :param n_classes: number of possible conditions
        :param emb_dim: dimension of the embedding space
        :return: keras model of generator
        """

        ## label input

        in_label=Input(shape=(1,))                               #input for condition the class
        li=Embedding(n_classes,emb_dim)(in_label)                #li stands for label input
        li=Dense(7*7)(li)
        li = Reshape((7, 7, 1))(li)                              #produce starting image compatible shapes

        #latent input

        in_latent=Input(shape=(latent_dim,))

        gen=Dense(7 * 7 * 64)(in_latent)
        gen=LeakyReLU(alpha=0.2)(gen)
        gen=Reshape((7, 7, 64))(gen)

        merge = Concatenate()([gen, li])

        x=Conv2DTranspose(64, (4, 4), strides=(2, 2), padding="same")(merge)
        x=LeakyReLU(alpha=0.2)(x)
        x=Conv2DTranspose(64, (4, 4), strides=(2, 2), padding="same")(x)
        x=LeakyReLU(alpha=0.2)(x)
        x=Conv2D(1, (7, 7), padding="same", activation="sigmoid")(x)

        model = Model([in_latent, in_label], x,name="generator")

        return model

    def build_gan(self):

        """
        Generative MODEL


        WARNING: If is intended to work with gan.train_on_batch() is needed to uncomment d_model.trainable=False

        :return: keras Generative model
        """

        g_model=self.generator
        d_model=self.discriminator

        #d_model.trainable=False   ##decomment if gan.train_on_batch is needed

        gen_noise,gen_label=g_model.input
        gen_output=g_model.output

        gan_output = d_model([gen_output, gen_label])

        model = Model([gen_noise, gen_label], gan_output,name="cGAN")

        return model

class CWGAN(keras.Model):
    """
    Conditional GAN version with wesserstein loss

    There are some differences with the basic GAN

    Loss:
    -----------


    The main difference is the loss function:

    The discriminator is a critic that tries to maximize the difference

    L=D(x)-D(G(z))
    while the generator will try to maximize the discriminator's output on synthetic images

    L=-D(G(z))

    Other Changes:
    ---------------

    The discriminator will be updated more frequently than generator

    The last layer of the discriminator will have a linear activation function

    Clipping of the weights update

    RMSProp as optimizer

    labels are -1 for real images and 1 for fake images!


    """
    def __init__(self,target_shape,latent_dim,d_steps=5,condition_shape=(1,),n_classes=10,emb_dim=50,encoder_architecture=[(0,128),[(0,256)]], decoder_architecture=[(0,128),[(0,256)]]):
        """

        Attributes
        ----------

        :param latent_dim: dimension of the latent space (i.e the number of random numbers required to generate an image)
        :param target_shape: tuple, shape of the image
        :param condition_shape: tuple, shape of the condition ex (1,) for mnist or integer labels
        :param emb_dim: int embedding dimensions
        :param n_classes: int number of possible classes
        :param discriminator: model
        :param generator : model
        :param latent_dim: dimension of the latent space (i.e the number of random numbers required to generate an image)
        :param encoder_architecture: list of tuple, len of list is the number of blocks, [(n_block_res,n_filters)..] for discriminator
        :param decoder_architecture: list of tuple, len of list is the number of blocks, [(n_block_res,n_filters)..] for generator


        Methods
        ---------
        build_discriminator : build a sequential Keras model to discriminate between real and fake images
        build_generator: build a sequential Keras model to generate images from noise though Conv2DTranspose layers.

        """
        super().__init__()

        self.target_shape = target_shape
        self.latent_dim = latent_dim
        self.condition_shape=condition_shape
        self.emb_dim=emb_dim
        self.n_classes=n_classes

        self.d_steps=d_steps

        #self.encoder_architecture=encoder_architecture
        #self.decoder_architecture=decoder_architecture



        self.discriminator = self.build_discriminator()
        self.generator = self.build_generator(latent_dim)

        #self.discriminator=Discriminator(target_shape,1,conv_layer_list=encoder_architecture,activation="linear")
        #self.generator=Decoder(target_shape,latent_dim,decoder_architecture)

        self.gan=self.build_gan()


    def compile(self, d_optimizer=tf.keras.optimizers.Adam(3e-4), g_optimizer=tf.keras.optimizers.Adam(3e-4), loss_fn=tf.keras.losses.BinaryCrossentropy(from_logits=True),d_accuracy=tf.keras.metrics.Accuracy()):
        """
        method to compile all modules of this model

        :param d_optimizer: optimizer for discriminator
        :param g_optimizer: optimizer for generator
        :param loss_fn:  loss function to use for discriminator and gan
        :param d_accuracy: metric to measure discriminator performances
        """

        super(CWGAN, self).compile(run_eagerly=True)
        self.d_optimizer = d_optimizer
        self.g_optimizer = g_optimizer


        self.discrimnator_loss = loss_fn
        self.d_accuracy=d_accuracy

    def wesserstein_loss(y_true, y_pred):

        #y_true_round=tf.math.round(y_true) #restore original labels

        #out_fake=tf.reduce_mean(y_pred[y_true_round==1.])
        #out_true=tf.reduce_mean(y_pred[y_true_round==-1.])


        """This should be equivalent to maximize D(x)-D(G(z))"""
        #return -(out_true-out_fake)
        return tf.reduce_mean(y_true*y_pred)



    def train(self,train_data,steps_per_epoch,epochs=10,val_data=None,validation_steps=None,log=True,callbacks=None):


        #callback stuff
        logs = {}

        if callbacks is not None:
            callbacks = tf.keras.callbacks.CallbackList(
                callbacks, add_history=True, model=self)

        if callbacks is not None:
            callbacks.on_train_begin(logs=logs)

        for epoch in (range(epochs)):
            print(f"[EPOCH] {epoch}/{epochs}")
            for _ in tqdm.tqdm(range(steps_per_epoch//self.d_steps)):
                #train the critic/discriminator
                d_loss_list=[]
                d_acc_list=[]
                g_loss_list=[]

                #train discriminator
                for x in train_data.take(self.d_steps):
                    bs=x.shape[0]
                    d_loss,d_acc=self.train_critic(x)


                    d_loss_list.append(d_loss)
                    d_acc_list.append(d_acc)

                    if log:
                        wandb.log({"d_loss":d_loss,"d_acc":d_acc})

                #train the generator through the gan with freezed discriminator weights


                g_loss=self.train_gan(bs)
                g_loss_list.append(g_loss)
                if log:
                    wandb.log({"g_loss": g_loss})
                logs={"d_loss":np.mean(d_loss_list),"d_acc":np.mean(d_acc_list),"g_loss":np.mean(g_loss_list)}

            if callbacks is not None:
                callbacks.on_epoch_end(epoch, logs=logs)

        if callbacks is not None:
            callbacks.on_train_end(logs=logs)

    def build_gan(self):
        # make weights in the critic not trainable
        for layer in self.discriminator.layers:
            if not isinstance(layer, BatchNormalization):
                layer.trainable = False

        condition_input=Input(self.condition_shape)
        in_label = Input(shape=(1,))  # input for condition the class
        in_latent = Input(shape=(self.latent_dim,))

        gen=self.generator([in_latent,in_label])
        cri=self.discriminator([gen,in_label])

        model=Model([in_latent,in_label],cri,name="gan")


        # connect them
        #model = Sequential()
        # add generator
        #model.add(self.generator)
        # add the critic
        #model.add(self.discriminator)
        # compile model
        opt = RMSprop(lr=0.00005)
        model.compile(loss=CWGAN.wesserstein_loss, optimizer=opt)
        return model


    def train_gan(self,bs):


        random_latent_vectors = tf.random.normal(shape=(bs, self.latent_dim))
        random_conditions = tf.random.uniform(shape=[bs,*self.condition_shape],minval=0,maxval=self.n_classes,dtype=tf.int32)

        misleading_labels= -tf.ones((bs,1))

        g_loss=	self.gan.train_on_batch([random_latent_vectors,random_conditions], misleading_labels)
        return g_loss





    def train_critic(self,x,noisy_labels=False):
        """Train the discriminator
        :param x: images
        :param noisy_labels: bool, if True add some noise to labels to further stabilize training
        """

        x,y=x #separate conditions
        batch_size = tf.shape(x)[0]

        ## step 1 train the discriminator with real and fake imgs

        # step 1.1 train on real images

        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))
        random_conditions = tf.random.uniform(shape=[batch_size,*self.condition_shape],minval=0,maxval=self.n_classes,dtype=tf.int32)


        # Decode them to fake images
        generated_images = self.generator([random_latent_vectors,random_conditions])

        # Combine them with real images
        combined_images = tf.concat([generated_images, x], axis=0)

        # Assemble labels discriminating real from fake images
        labels = tf.concat(
            [tf.ones((batch_size, 1)), -tf.ones((batch_size, 1))], axis=0
        )

        conditions=tf.concat([random_conditions,y],axis=0)

        if noisy_labels:
            # Add random noise to the labels - important trick!
            labels += 0.05 * tf.random.uniform(labels.shape)

        c_loss,c_acc=self.discriminator.train_on_batch([combined_images,conditions],labels)
        return c_loss,c_acc



    def build_discriminator(self):
        """
        discriminator model

        :return: discriminator model
        """

        dims=self.target_shape
        in_label=Input(shape=self.condition_shape)                  #input for condition the class
        li=Embedding(self.n_classes,self.emb_dim)(in_label)              #li stands for label input
        li=Dense(np.prod(dims))(li)
        li = Reshape((dims[0], dims[1], 1))(li)        #produce image compatible shapes

        # image input
        in_image = Input(shape=dims)

        merge = Concatenate()([in_image, li])  # concatenate into a multichannel image

        const=ClipConstraint(0.01)
        # Create the discriminator

        x=Conv2D(256, (3, 3), strides=(2, 2), padding="same",kernel_constraint=const)(merge)
        x=LeakyReLU(alpha=0.2)(x)
        x=BatchNormalization()(x)
        x=Conv2D(512, (3, 3), strides=(2, 2), padding="same",kernel_constraint=const)(x)
        x=LeakyReLU(alpha=0.2)(x)
        x=BatchNormalization()(x)
        x=GlobalMaxPooling2D()(x)
        x=Dense(1)(x)

        discriminator=Model([in_image,in_label],x,name="discriminator")

        opt = RMSprop(lr=0.00005)
        discriminator.compile(loss=CWGAN.wesserstein_loss, optimizer=opt,metrics="accuracy")
        return discriminator


    def build_generator(self,latent_dim=128):
        """

        :param latent_dim: dimension of the latent space
        :return: generator model
        """
        ## label input

        in_label = Input(shape=(1,))  # input for condition the class
        li = Embedding(self.n_classes, self.emb_dim)(in_label)  # li stands for label input
        li = Dense(7 * 7)(li)
        li = Reshape((7, 7, 1))(li)  # produce starting image compatible shapes

        # latent input

        in_latent = Input(shape=(latent_dim,))

        gen = Dense(7 * 7 * 64)(in_latent)
        gen = LeakyReLU(alpha=0.2)(gen)
        gen = Reshape((7, 7, 64))(gen)

        merge = Concatenate()([gen, li])

        x = Conv2DTranspose(128, (4, 4), strides=(2, 2), padding="same")(merge)
        x = LeakyReLU(alpha=0.2)(x)
        x = Conv2DTranspose(64, (4, 4), strides=(2, 2), padding="same")(x)
        x = LeakyReLU(alpha=0.2)(x)
        x = Conv2D(1, (7, 7), padding="same", activation="sigmoid")(x)

        model = Model([in_latent, in_label], x, name="generator")

        return model


