{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2201223a",
   "metadata": {},
   "source": [
    "## Train GAN on CelebA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b56fcd6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-14 14:42:02.155017: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-14 14:42:07.079993: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 72677 MB memory:  -> device: 0, name: NVIDIA A100-SXM-80GB, pci bus id: 0000:07:00.0, compute capability: 8.0\n",
      "2021-12-14 14:42:07.081910: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 72 MB memory:  -> device: 1, name: NVIDIA A100-SXM-80GB, pci bus id: 0000:0f:00.0, compute capability: 8.0\n",
      "2021-12-14 14:42:07.084272: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 79131 MB memory:  -> device: 2, name: NVIDIA A100-SXM-80GB, pci bus id: 0000:47:00.0, compute capability: 8.0\n",
      "2021-12-14 14:42:07.085876: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 79131 MB memory:  -> device: 3, name: NVIDIA A100-SXM-80GB, pci bus id: 0000:4e:00.0, compute capability: 8.0\n",
      "2021-12-14 14:42:07.087498: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:4 with 79131 MB memory:  -> device: 4, name: NVIDIA A100-SXM-80GB, pci bus id: 0000:87:00.0, compute capability: 8.0\n",
      "2021-12-14 14:42:07.089103: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:5 with 79131 MB memory:  -> device: 5, name: NVIDIA A100-SXM-80GB, pci bus id: 0000:90:00.0, compute capability: 8.0\n",
      "2021-12-14 14:42:07.090718: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:6 with 79131 MB memory:  -> device: 6, name: NVIDIA A100-SXM-80GB, pci bus id: 0000:b7:00.0, compute capability: 8.0\n",
      "2021-12-14 14:42:07.092332: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:7 with 2970 MB memory:  -> device: 7, name: NVIDIA A100-SXM-80GB, pci bus id: 0000:bd:00.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from classes.GAN import GAN\n",
    "from utils.callbacks import WandbImagesVAE, SaveGeneratorWeights, SaveVAEWeights, WandbVAECallback, WandbImagesGAN, \\\n",
    "    SaveGANWeights\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4db1e348",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c804753",
   "metadata": {},
   "source": [
    "### Model definition\n",
    "And initial configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "85edd608",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    }
   ],
   "source": [
    "wandb.login()\n",
    "\n",
    "latent_dim=512\n",
    "input_shape=(128,128,3)\n",
    "\n",
    "encoder_architecture=[(0,32),(0,64),(1,128),(1,128),(1,256)]\n",
    "decoder_architecture=[(0,256),(0,128),(1,128),(1,64),(1,32)]\n",
    "\n",
    "g=GAN(input_shape,\n",
    "      latent_dim=latent_dim,\n",
    "      encoder_architecture=encoder_architecture,\n",
    "      decoder_architecture=decoder_architecture)\n",
    "\n",
    "\n",
    "config={\"dataset\":\"celebA\", \"type\":\"GAN\",\"encoder_architecture\":encoder_architecture,\"decoder_architecture\":decoder_architecture}\n",
    "config.update(g.get_dict())\n",
    "\n",
    "\n",
    "images_dir=r\"/home/matteo/NeuroGEN/Dataset/Img/img_align_celeba\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9569569f",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddf56698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " latent_input (InputLayer)   [(None, 512)]             0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1024)              525312    \n",
      "                                                                 \n",
      " reshape_1 (Reshape)         (None, 4, 4, 64)          0         \n",
      "                                                                 \n",
      " leaky_re_lu_10 (LeakyReLU)  (None, 4, 4, 64)          0         \n",
      "                                                                 \n",
      " conv_transpose_res_block_5   (None, 8, 8, 256)        147712    \n",
      " (ConvTransposeResBlock)                                         \n",
      "                                                                 \n",
      " conv_transpose_res_block_6   (None, 16, 16, 128)      295040    \n",
      " (ConvTransposeResBlock)                                         \n",
      "                                                                 \n",
      " conv_transpose_res_block_7   (None, 32, 32, 128)      460288    \n",
      " (ConvTransposeResBlock)                                         \n",
      "                                                                 \n",
      " conv_transpose_res_block_8   (None, 64, 64, 64)       152320    \n",
      " (ConvTransposeResBlock)                                         \n",
      "                                                                 \n",
      " conv_transpose_res_block_9   (None, 128, 128, 32)     38272     \n",
      " (ConvTransposeResBlock)                                         \n",
      "                                                                 \n",
      " conv2d_transpose_11 (Conv2D  (None, 128, 128, 3)      867       \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,619,811\n",
      "Trainable params: 1,618,915\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "g.generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9bf976",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5119fdcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"discriminator\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " image_input (InputLayer)    [(None, 128, 128, 3)]     0         \n",
      "                                                                 \n",
      " conv_res_block_5 (ConvResBl  (None, 64, 64, 32)       896       \n",
      " ock)                                                            \n",
      "                                                                 \n",
      " conv_res_block_6 (ConvResBl  (None, 32, 32, 64)       18496     \n",
      " ock)                                                            \n",
      "                                                                 \n",
      " conv_res_block_7 (ConvResBl  (None, 16, 16, 128)      386560    \n",
      " ock)                                                            \n",
      "                                                                 \n",
      " conv_res_block_8 (ConvResBl  (None, 8, 8, 128)        460288    \n",
      " ock)                                                            \n",
      "                                                                 \n",
      " conv_res_block_9 (ConvResBl  (None, 4, 4, 256)        1543168   \n",
      " ock)                                                            \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 4097      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,413,505\n",
      "Trainable params: 2,411,457\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "g.discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea25de47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:36ddbher) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 8585... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "</div><div class=\"wandb-col\">\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">volcanic-plasma-455</strong>: <a href=\"https://wandb.ai/matteoferrante/TorVergataExperiment-Generative/runs/36ddbher\" target=\"_blank\">https://wandb.ai/matteoferrante/TorVergataExperiment-Generative/runs/36ddbher</a><br/>\n",
       "Find logs at: <code>./wandb/run-20211214_144215-36ddbher/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:36ddbher). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/matteoferrante/TorVergataExperiment-Generative/runs/3eaaqcmn\" target=\"_blank\">youthful-forest-456</a></strong> to <a href=\"https://wandb.ai/matteoferrante/TorVergataExperiment-Generative\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/matteoferrante/TorVergataExperiment-Generative/runs/3eaaqcmn?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7efbd0730580>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#other important definitions\n",
    "\n",
    "EPOCHS=250\n",
    "BS=512\n",
    "INIT_LR=5e-5\n",
    "\n",
    "config[\"epochs\"]=EPOCHS\n",
    "config[\"BS\"]=BS\n",
    "config[\"init_lr\"]=INIT_LR\n",
    "\n",
    "config[\"nota\"]=\"DGX\"\n",
    "\n",
    "wandb.init(project=\"TorVergataExperiment-Generative\",config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73188fe4",
   "metadata": {},
   "source": [
    "## Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8d0ca19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading image paths...\n",
      "[TRAINING]\t 138545\n",
      "[VALIDATION]\t 17318\n",
      "[TEST]\t\t 17319\n"
     ]
    }
   ],
   "source": [
    "def load_images(imagePath):\n",
    "    # read the image from disk, decode it, resize it, and scale the\n",
    "    # pixels intensities to the range [0, 1]\n",
    "    image = tf.io.read_file(imagePath)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, (128, 128)) / 255.0\n",
    "\n",
    "    # eventually load other information like attributes here\n",
    "\n",
    "    # return the image and the extra info\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "print(\"[INFO] loading image paths...\")\n",
    "imagePaths = list(paths.list_images(images_dir))\n",
    "\n",
    "\n",
    "train_len=int(0.8*len(imagePaths))\n",
    "val_len=int(0.1*len(imagePaths))\n",
    "test_len=int(0.1*len(imagePaths))\n",
    "\n",
    "train_imgs=imagePaths[:train_len]                                #      80% for training\n",
    "val_imgs=imagePaths[train_len:train_len+val_len]                 #      10% for validation\n",
    "test_imgs=imagePaths[train_len+val_len:]                         #      10% for testing\n",
    "\n",
    "print(f\"[TRAINING]\\t {len(train_imgs)}\\n[VALIDATION]\\t {len(val_imgs)}\\n[TEST]\\t\\t {len(test_imgs)}\")\n",
    "\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_imgs)\n",
    "train_dataset = (train_dataset\n",
    "    .shuffle(1024)\n",
    "    .map(load_images)\n",
    "    .cache()\n",
    "    .repeat()\n",
    "    .batch(BS)\n",
    ")\n",
    "\n",
    "ts=len(train_imgs)//BS\n",
    "\n",
    "##VALIDATION\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(val_imgs)\n",
    "val_dataset = (val_dataset\n",
    "    .shuffle(1024)\n",
    "    .map(load_images)\n",
    "    .cache()\n",
    "    .repeat()\n",
    "    .batch(BS)\n",
    ")\n",
    "\n",
    "vs=len(val_imgs)//BS\n",
    "\n",
    "## TEST\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(test_imgs)\n",
    "test_dataset = (test_dataset\n",
    "    .shuffle(1024)\n",
    "    .map(load_images)\n",
    "    .cache()\n",
    "    .batch(BS)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156708b1",
   "metadata": {},
   "source": [
    "## Compile\n",
    "And set callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "811dda67",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"models/gan\",exist_ok=True)\n",
    "model_check=SaveGANWeights(filepath=\"models/gan\")\n",
    "\n",
    "g.compile()\n",
    "\n",
    "\n",
    "try:                                  #workaround to use Wandbcallback at first attempt\n",
    "    wb=WandbCallback()\n",
    "except:\n",
    "    wb=WandbCallback()\n",
    "    \n",
    "    \n",
    "callbacks=[\n",
    "    WandbImagesGAN(target_shape=(128,128,3)),\n",
    "    wb,\n",
    "    model_check,\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6264d2a",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192dad5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n",
      "Epoch 1/250\n",
      "270/270 [==============================] - ETA: 0s - d_loss: -22013.5248 - g_loss: 15396427.7773 - d_acc: 0.1121"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-14 14:52:35.032592: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270/270 [==============================] - 144s 522ms/step - d_loss: -21917.7915 - g_loss: 15339709.9256 - d_acc: 0.1118 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/250\n",
      "270/270 [==============================] - ETA: 0s - d_loss: 4241.5482 - g_loss: 11933.4230 - d_acc: 1.8454e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-14 14:54:57.222151: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270/270 [==============================] - 142s 526ms/step - d_loss: 4241.7092 - g_loss: 12075.1970 - d_acc: 1.8426e-05 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/250\n",
      "270/270 [==============================] - ETA: 0s - d_loss: 2231.5099 - g_loss: 13465.0404 - d_acc: 0.0000e+00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-14 14:57:08.593063: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270/270 [==============================] - 131s 486ms/step - d_loss: 2224.5936 - g_loss: 13438.6701 - d_acc: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 2.9593e-05\n",
      "Epoch 4/250\n",
      "270/270 [==============================] - ETA: 0s - d_loss: 1459.0161 - g_loss: 8828.3858 - d_acc: 2.4984e-06"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-14 14:59:21.164362: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270/270 [==============================] - 133s 491ms/step - d_loss: 1455.2416 - g_loss: 8817.3873 - d_acc: 2.5159e-06 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/250\n",
      "270/270 [==============================] - ETA: 0s - d_loss: 428.7606 - g_loss: 6246.8268 - d_acc: 9.2429e-07"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-14 15:01:35.010339: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270/270 [==============================] - 134s 496ms/step - d_loss: 430.7448 - g_loss: 6287.4113 - d_acc: 9.3423e-07 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/250\n",
      "270/270 [==============================] - ETA: 0s - d_loss: 386.4000 - g_loss: 5986.7837 - d_acc: 1.4778e-06"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-14 15:03:50.429978: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270/270 [==============================] - 135s 502ms/step - d_loss: 386.6181 - g_loss: 5990.8659 - d_acc: 1.5124e-06 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/250\n",
      "270/270 [==============================] - ETA: 0s - d_loss: 707.0102 - g_loss: 5374.5471 - d_acc: 4.5538e-07"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-14 15:06:04.133931: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270/270 [==============================] - 134s 495ms/step - d_loss: 705.4385 - g_loss: 5375.0887 - d_acc: 4.6704e-07 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/250\n",
      "270/270 [==============================] - ETA: 0s - d_loss: 197.3086 - g_loss: 3488.4167 - d_acc: 9.7193e-07"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-14 15:08:16.231103: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270/270 [==============================] - 132s 489ms/step - d_loss: 196.8453 - g_loss: 3489.0295 - d_acc: 9.9504e-07 - val_loss: 0.0000e+00 - val_accuracy: 2.9593e-05\n",
      "Epoch 9/250\n",
      "270/270 [==============================] - ETA: 0s - d_loss: 275.3025 - g_loss: 3304.7914 - d_acc: 3.6835e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-14 15:10:27.831580: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270/270 [==============================] - 132s 487ms/step - d_loss: 274.6047 - g_loss: 3300.0794 - d_acc: 3.6779e-05 - val_loss: 0.0000e+00 - val_accuracy: 2.9593e-05\n",
      "Epoch 10/250\n",
      "270/270 [==============================] - ETA: 0s - d_loss: 157.8836 - g_loss: 3149.4737 - d_acc: 1.2174e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-14 15:12:39.330804: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270/270 [==============================] - 132s 487ms/step - d_loss: 157.7319 - g_loss: 3149.6790 - d_acc: 1.2182e-05 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/250\n",
      "270/270 [==============================] - ETA: 0s - d_loss: 129.6205 - g_loss: 3190.0699 - d_acc: 3.7352e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-14 15:14:51.034438: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270/270 [==============================] - 132s 488ms/step - d_loss: 129.2832 - g_loss: 3184.8735 - d_acc: 3.7321e-05 - val_loss: 0.0000e+00 - val_accuracy: 2.9593e-05\n",
      "Epoch 12/250\n",
      "270/270 [==============================] - ETA: 0s - d_loss: 145.9760 - g_loss: 2500.7682 - d_acc: 4.6190e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-14 15:17:02.045799: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270/270 [==============================] - 131s 485ms/step - d_loss: 145.8026 - g_loss: 2496.2061 - d_acc: 4.6100e-05 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/250\n",
      "270/270 [==============================] - ETA: 0s - d_loss: 84.2460 - g_loss: 2431.3094 - d_acc: 5.8960e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-14 15:19:13.350444: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270/270 [==============================] - 131s 486ms/step - d_loss: 84.0019 - g_loss: 2430.4536 - d_acc: 5.8929e-05 - val_loss: 0.0000e+00 - val_accuracy: 2.9593e-05\n",
      "Epoch 14/250\n",
      "270/270 [==============================] - ETA: 0s - d_loss: 121.4190 - g_loss: 2127.4312 - d_acc: 2.5295e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-14 15:21:25.314631: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270/270 [==============================] - 132s 489ms/step - d_loss: 121.0531 - g_loss: 2127.2390 - d_acc: 2.5295e-05 - val_loss: 0.0000e+00 - val_accuracy: 5.9186e-05\n",
      "Epoch 15/250\n",
      "270/270 [==============================] - ETA: 0s - d_loss: 31.4712 - g_loss: 1648.5473 - d_acc: 3.1455e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-14 15:23:36.915965: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270/270 [==============================] - 132s 488ms/step - d_loss: 31.3961 - g_loss: 1648.8028 - d_acc: 3.1472e-05 - val_loss: 0.0000e+00 - val_accuracy: 1.1837e-04\n",
      "Epoch 16/250\n",
      "270/270 [==============================] - ETA: 0s - d_loss: 45.2291 - g_loss: 1822.2699 - d_acc: 7.1616e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-14 15:25:48.209439: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270/270 [==============================] - 131s 486ms/step - d_loss: 45.0893 - g_loss: 1823.1572 - d_acc: 7.1565e-05 - val_loss: 0.0000e+00 - val_accuracy: 1.4796e-04\n",
      "Epoch 17/250\n",
      "270/270 [==============================] - ETA: 0s - d_loss: 54.1875 - g_loss: 1732.9711 - d_acc: 6.4810e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-14 15:27:59.522089: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270/270 [==============================] - 132s 487ms/step - d_loss: 54.1605 - g_loss: 1736.7940 - d_acc: 6.4784e-05 - val_loss: 0.0000e+00 - val_accuracy: 5.9186e-05\n",
      "Epoch 18/250\n",
      "270/270 [==============================] - ETA: 0s - d_loss: 21.9349 - g_loss: 1552.4861 - d_acc: 3.0329e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-14 15:30:10.401112: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270/270 [==============================] - 131s 484ms/step - d_loss: 21.8951 - g_loss: 1552.2029 - d_acc: 3.0350e-05 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/250\n",
      "270/270 [==============================] - ETA: 0s - d_loss: 44.3445 - g_loss: 1506.3174 - d_acc: 6.1838e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-14 15:32:21.296400: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270/270 [==============================] - 131s 485ms/step - d_loss: 44.2435 - g_loss: 1508.5674 - d_acc: 6.1810e-05 - val_loss: 0.0000e+00 - val_accuracy: 1.4796e-04\n",
      "Epoch 20/250\n",
      "270/270 [==============================] - ETA: 0s - d_loss: 63.4678 - g_loss: 1227.1965 - d_acc: 7.3148e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-14 15:34:31.618430: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270/270 [==============================] - 130s 483ms/step - d_loss: 63.2945 - g_loss: 1223.8855 - d_acc: 7.3132e-05 - val_loss: 0.0000e+00 - val_accuracy: 8.8778e-05\n",
      "Epoch 21/250\n",
      "270/270 [==============================] - ETA: 0s - d_loss: 31.6798 - g_loss: 630.9371 - d_acc: 1.0356e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-14 15:36:41.566943: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270/270 [==============================] - 130s 481ms/step - d_loss: 31.5890 - g_loss: 630.7125 - d_acc: 1.0354e-04 - val_loss: 0.0000e+00 - val_accuracy: 1.4796e-04\n",
      "Epoch 22/250\n",
      "270/270 [==============================] - ETA: 0s - d_loss: 20.6450 - g_loss: 662.3336 - d_acc: 1.3018e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-14 15:38:51.920777: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270/270 [==============================] - 130s 483ms/step - d_loss: 20.5914 - g_loss: 662.3855 - d_acc: 1.3015e-04 - val_loss: 0.0000e+00 - val_accuracy: 2.9593e-05\n",
      "Epoch 23/250\n",
      "270/270 [==============================] - ETA: 0s - d_loss: 5.2411 - g_loss: 662.7536 - d_acc: 7.5301e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-14 15:41:01.581936: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270/270 [==============================] - 130s 480ms/step - d_loss: 5.2583 - g_loss: 662.4843 - d_acc: 7.5423e-05 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/250\n",
      "270/270 [==============================] - ETA: 0s - d_loss: 11.3193 - g_loss: 705.1530 - d_acc: 1.7948e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-14 15:43:11.435417: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270/270 [==============================] - 130s 481ms/step - d_loss: 11.2820 - g_loss: 705.7224 - d_acc: 1.7939e-04 - val_loss: 0.0000e+00 - val_accuracy: 2.6634e-04\n",
      "Epoch 25/250\n",
      "270/270 [==============================] - ETA: 0s - d_loss: 2.9628 - g_loss: 688.8198 - d_acc: 9.2804e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-14 15:45:21.491036: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270/270 [==============================] - 130s 482ms/step - d_loss: 2.9516 - g_loss: 688.6345 - d_acc: 9.2849e-05 - val_loss: 0.0000e+00 - val_accuracy: 2.9593e-05\n",
      "Epoch 26/250\n",
      "270/270 [==============================] - ETA: 0s - d_loss: 22.9641 - g_loss: 582.0372 - d_acc: 1.1052e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-14 15:47:32.511940: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270/270 [==============================] - 131s 485ms/step - d_loss: 22.8957 - g_loss: 582.3532 - d_acc: 1.1062e-04 - val_loss: 0.0000e+00 - val_accuracy: 1.7756e-04\n",
      "Epoch 27/250\n",
      "270/270 [==============================] - ETA: 0s - d_loss: 3.9300 - g_loss: 568.8523 - d_acc: 1.3639e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-14 15:49:45.045933: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270/270 [==============================] - 133s 491ms/step - d_loss: 3.9221 - g_loss: 569.0277 - d_acc: 1.3647e-04 - val_loss: 0.0000e+00 - val_accuracy: 2.6634e-04\n",
      "Epoch 28/250\n",
      "270/270 [==============================] - ETA: 0s - d_loss: 3.3726 - g_loss: 582.5078 - d_acc: 1.6268e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-14 15:51:56.802716: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270/270 [==============================] - 132s 488ms/step - d_loss: 3.3602 - g_loss: 582.9295 - d_acc: 1.6280e-04 - val_loss: 0.0000e+00 - val_accuracy: 3.8471e-04\n",
      "Epoch 29/250\n",
      "270/270 [==============================] - ETA: 0s - d_loss: 37.3338 - g_loss: 538.1000 - d_acc: 1.7926e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-14 15:54:08.634127: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270/270 [==============================] - 132s 488ms/step - d_loss: 37.5078 - g_loss: 538.2860 - d_acc: 1.7933e-04 - val_loss: 0.0000e+00 - val_accuracy: 1.7756e-04\n",
      "Epoch 30/250\n",
      "270/270 [==============================] - ETA: 0s - d_loss: 9.3315 - g_loss: 190.2759 - d_acc: 1.9985e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-14 15:56:20.114130: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270/270 [==============================] - 131s 487ms/step - d_loss: 9.3112 - g_loss: 189.9462 - d_acc: 2.0011e-04 - val_loss: 0.0000e+00 - val_accuracy: 2.9593e-04\n",
      "Epoch 31/250\n",
      "268/270 [============================>.] - ETA: 0s - d_loss: 1.7585 - g_loss: 149.2164 - d_acc: 3.6462e-04"
     ]
    }
   ],
   "source": [
    "#just to build the model\n",
    "x=np.zeros((BS,*input_shape))\n",
    "g(x)\n",
    "\n",
    "g.fit(train_dataset,validation_data=test_dataset,steps_per_epoch=ts,validation_steps=vs,epochs=EPOCHS,callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974cff4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
