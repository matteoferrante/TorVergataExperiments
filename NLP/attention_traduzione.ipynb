{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import typing\n",
    "from typing import Any, Tuple\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "import tensorflow_text as tf_text\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"data/ita.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset and separate input from target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    \"\"\"\n",
    "    load the files and return coupled list of input target\n",
    "    \"\"\"\n",
    "    with open(path, \"r\",encoding=\"utf-8\") as f:\n",
    "        text =f.read() \n",
    "\n",
    "    lines = text.splitlines()\n",
    "    pairs = [line.split('\\t') for line in lines]\n",
    "\n",
    "    inp = [inp for targ, inp,attr in pairs]\n",
    "    targ = [targ for targ, inp,attr in pairs]\n",
    "\n",
    "    return targ, inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "targ, inp = load_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(inp)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inp, targ)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we want to include all preprocessing inside the model \n",
    "\n",
    "In order to be able to export it as tf_saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_lower_and_split_punct(text):\n",
    "    # Split accecented characters.\n",
    "    text = tf_text.normalize_utf8(text, 'NFKD')\n",
    "    text = tf.strings.lower(text)\n",
    "    # Keep space, a to z, and select punctuation.\n",
    "    text = tf.strings.regex_replace(text, '[^ a-z.?!,]', '')\n",
    "    # Add spaces around punctuation.\n",
    "    text = tf.strings.regex_replace(text, '[.?!,]', r' \\0 ')\n",
    "    # Strip whitespace.\n",
    "    text = tf.strings.strip(text)\n",
    "\n",
    "    text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = tf.constant('Ciao, tutto bene?')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ciao, tutto bene?\n",
      "[START] ciao ,  tutto bene ? [END]\n"
     ]
    }
   ],
   "source": [
    "print(example_text.numpy().decode())\n",
    "print(tf_lower_and_split_punct(example_text).numpy().decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Vectorization\n",
    "for both input and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_size = 5000\n",
    "\n",
    "input_text_processor = preprocessing.TextVectorization(\n",
    "    standardize=tf_lower_and_split_punct,\n",
    "    max_tokens=max_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', '[START]', '[END]', '.', 'tom', '?', 'non', 'e', 'di']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text_processor.adapt(inp)\n",
    "\n",
    "# Here are the first 10 words from the vocabulary:\n",
    "input_text_processor.get_vocabulary()[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', '[START]', '[END]', '.', 'you', 'tom', 'i', 'to', '?']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_text_processor = preprocessing.TextVectorization(\n",
    "    standardize=tf_lower_and_split_punct,\n",
    "    max_tokens=max_vocab_size)\n",
    "\n",
    "output_text_processor.adapt(targ)\n",
    "output_text_processor.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'Penso di avere una linea di febbre.' b'Qualsiasi cosa va qui.'\n",
      " b'Ho fatto vedere a Tom come si fa.'\n",
      " b\"La giustizia si trova in un'aula di giustizia.\"\n",
      " b'Vi aspetter\\xc3\\xb2 nella mia camera.'\n",
      " b'Tom \\xc3\\xa8 nato nello spazio.' b'\\xc3\\x88 la proprietaria.'\n",
      " b'Snocciolammo delle ciliegie per la torta.'\n",
      " b'Sono venuti tutti tranne Tom.' b'Io non me la ricordo.'\n",
      " b'Lei \\xc3\\xa8 complesso.' b'Non pu\\xc3\\xb2 pi\\xc3\\xb9 farlo.'\n",
      " b'Ho visto alcune persone nuotare nel fiume.'\n",
      " b'So che mi sta osservando.' b'Voi siete oberati di lavoro.'\n",
      " b'Ha una camera pi\\xc3\\xb9 economica?' b'Volevo venire.'\n",
      " b'Era di buonumore.' b'La forchetta cadde dal tavolo.'\n",
      " b'La camera \\xc3\\xa8 troppo grande.' b'Le sparer\\xc3\\xb2.'\n",
      " b\"Lui fa bollire dell'acqua in una caffettiera.\"\n",
      " b'Ci hanno dato la loro parola.' b'Puoi metterla ovunque.'\n",
      " b'Lascia i libri e le riviste cos\\xc3\\xac come sono.'\n",
      " b'Alain ha concentrato tutte le sue attenzioni sulla sua gatta.'\n",
      " b\"Tom ha bisogno di un po' di aiuto.\" b'Io presi un cellulare nuovo.'\n",
      " b'Tom mi trover\\xc3\\xa0.' b'Io volevo che Tom partisse.' b'Sarai libero.'\n",
      " b\"Dov'\\xc3\\xa8 lo zoo?\" b'Non mi ascolter\\xc3\\xa0 nessuno.'\n",
      " b'Posso consigliare un buon avvocato.'\n",
      " b\"Dov'\\xc3\\xa8 la mia borsa da golf?\"\n",
      " b'Tom sar\\xc3\\xa0 qua tra un minuto.' b'Verr\\xc3\\xa0 presto.'\n",
      " b'Non sar\\xc3\\xa0 un problema.'\n",
      " b'Questi sono gli occhiali che stava cercando?'\n",
      " b'Di solito non bevo tequila.' b'Tom si \\xc3\\xa8 divertito.'\n",
      " b'Non posso suonare il piano.'\n",
      " b'Continu\\xc3\\xb2 a piangere per tutta la notte.' b'Io sono un monaco.'\n",
      " b'Io sospetto che Tom sia coinvolto.'\n",
      " b'Voi quando sarete di ritorno in Italia?'\n",
      " b'Mi sono fatta tagliare i capelli ieri.'\n",
      " b'La polizia non ha creduto alla storia di Tom.'\n",
      " b'Tom \\xc3\\xa8 stato decisivo.' b'Dia voce alla sua opinione.'\n",
      " b'Tom \\xc3\\xa8 inarrestabile.' b'La folla applaud\\xc3\\xac.'\n",
      " b'Io e Tom abbiamo intenzione di cantare assieme.'\n",
      " b'Io devo ancora pagare il mio affitto.' b'Sono stupita.'\n",
      " b'Non provatela neanche.' b'Tom sta aspettando un amico.'\n",
      " b'Non la disturber\\xc3\\xb2 pi\\xc3\\xb9 stasera.'\n",
      " b'Ora vivo in una casa molto piccola.'\n",
      " b'Mi disse che stava andando in Italia.'\n",
      " b'Il destino di Tom sar\\xc3\\xa0 deciso da una giuria.'\n",
      " b'Tu dovresti essere molto felice.'\n",
      " b'Qual \\xc3\\xa8 il tuo personaggio preferito dei cartoni Disney?'\n",
      " b'Ho perso il libro che mi ha prestato.'], shape=(64,), dtype=string)\n",
      "tf.Tensor(\n",
      "[b\"I think I've got a touch of fever.\" b'Anything goes here.'\n",
      " b'I showed Tom how to do it.' b'Justice is found in a courtroom.'\n",
      " b'I will be waiting for you in my room.' b'Tom was born in outer space.'\n",
      " b\"You're the owner.\" b'We pitted cherries for the pie.'\n",
      " b'Everyone but Tom came.' b\"I don't remember it.\" b\"You're complex.\"\n",
      " b\"You can't do that anymore.\" b'I saw some people swimming in the river.'\n",
      " b'I know he is watching me.' b'You are overworked.'\n",
      " b'Do you have a cheaper room?' b'I wanted to come.'\n",
      " b'He was in good spirits.' b'The fork fell off the table.'\n",
      " b'The room is too big.' b'I will shoot her.'\n",
      " b'He boils water in a coffee pot.' b'They gave us their word.'\n",
      " b'You can put it anywhere.' b'Leave the books and magazines as they are.'\n",
      " b'Alain focused all his attention on his cat.' b'Tom needs some help.'\n",
      " b'I got a new cellphone.' b'Tom will find me.' b'I wanted Tom to leave.'\n",
      " b\"You'll be free.\" b\"Where's the zoo?\" b'No one will listen to me.'\n",
      " b'I can recommend a good lawyer.' b\"Where's my golf bag?\"\n",
      " b'Tom will be here in a minute.' b'He will come soon.'\n",
      " b\"That won't be a problem.\"\n",
      " b'Are these the glasses you were looking for?'\n",
      " b\"I don't usually drink tequila.\" b'Tom had fun.'\n",
      " b\"I can't play the piano.\" b'She kept crying all night long.'\n",
      " b\"I'm a monk.\" b'I suspect Tom is involved.'\n",
      " b'When will you be back in Italy?' b'I got my hair cut yesterday.'\n",
      " b\"The police didn't believe Tom's story.\" b'Tom has been decisive.'\n",
      " b'Voice your opinion.' b'Tom is unstoppable.' b'The crowd clapped.'\n",
      " b'Tom and I plan to sing together.' b'I still need to pay my rent.'\n",
      " b\"I'm astonished.\" b\"Don't even try it.\" b'Tom is waiting for a friend.'\n",
      " b\"I won't bother you anymore tonight.\"\n",
      " b'I now live in a very small house.'\n",
      " b'He told me that he was going to Italy.'\n",
      " b\"Tom's fate will be decided by a jury.\" b'You should be very happy.'\n",
      " b\"Who's your favorite Disney character?\" b'I lost the book you lent me.'], shape=(64,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for x,y in dataset.take(1):\n",
    "    print(x)\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_tokens = input_text_processor(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'Penso di avere una linea di febbre.', shape=(), dtype=string)\n",
      "tf.Tensor([   2   59    9  178   23 3167    9 2032    4    3    0    0    0], shape=(13,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(x[0])\n",
    "print(example_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 256\n",
    "units = 1024\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "Pass the tokens, it learns some embeddings of those tokens and then pass them through a GRU RNN to process these vectors. It returns the sequence and the state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, input_vocab_size, embedding_dim, enc_units):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.enc_units = enc_units\n",
    "        self.input_vocab_size = input_vocab_size\n",
    "\n",
    "        # The embedding layer converts tokens to vectors\n",
    "        self.embedding = tf.keras.layers.Embedding(self.input_vocab_size,\n",
    "                                                   embedding_dim)\n",
    "\n",
    "        # The GRU RNN layer processes those vectors sequentially.\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       # Return the sequence and state\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, tokens, state=None):\n",
    "        #shape_checker = ShapeChecker()\n",
    "        #shape_checker(tokens, ('batch', 's'))\n",
    "\n",
    "        # 2. The embedding layer looks up the embedding for each token.\n",
    "        vectors = self.embedding(tokens)\n",
    "        #shape_checker(vectors, ('batch', 's', 'embed_dim'))\n",
    "\n",
    "        # 3. The GRU processes the embedding sequence.\n",
    "        #    output shape: (batch, s, enc_units)\n",
    "        #    state shape: (batch, enc_units)\n",
    "        output, state = self.gru(vectors, initial_state=state)\n",
    "        #shape_checker(output, ('batch', 's', 'enc_units'))\n",
    "        #shape_checker(state, ('batch', 'enc_units'))\n",
    "\n",
    "        # 4. Returns the new sequence and its state.\n",
    "        return output, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch, shape (batch): (64,)\n",
      "Input batch tokens, shape (batch, s): (64, 13)\n",
      "Encoder output, shape (batch, s, units): (64, 13, 1024)\n",
      "Encoder state, shape (batch, units): (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "# Convert the input text to tokens.\n",
    "example_tokens = input_text_processor(x)\n",
    "\n",
    "# Encode the input sequence.\n",
    "encoder = Encoder(input_text_processor.vocabulary_size(),\n",
    "                  embedding_dim, units)\n",
    "example_enc_output, example_enc_state = encoder(example_tokens)\n",
    "\n",
    "print(f'Input batch, shape (batch): {x.shape}')\n",
    "print(f'Input batch tokens, shape (batch, s): {example_tokens.shape}')\n",
    "print(f'Encoder output, shape (batch, s, units): {example_enc_output.shape}')\n",
    "print(f'Encoder state, shape (batch, units): {example_enc_state.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        # For Eqn. (4), the  Bahdanau attention\n",
    "        self.W1 = tf.keras.layers.Dense(units, use_bias=False)\n",
    "        self.W2 = tf.keras.layers.Dense(units, use_bias=False)\n",
    "\n",
    "        self.attention = tf.keras.layers.AdditiveAttention()\n",
    "\n",
    "    def call(self, query, value, mask):\n",
    "    \n",
    "        # From Eqn. (4), `W1@ht`.\n",
    "        w1_query = self.W1(query)\n",
    "\n",
    "        # From Eqn. (4), `W2@hs`.\n",
    "        w2_key = self.W2(value)\n",
    "\n",
    "        query_mask = tf.ones(tf.shape(query)[:-1], dtype=bool)\n",
    "        value_mask = mask\n",
    "\n",
    "        context_vector, attention_weights = self.attention(\n",
    "            inputs = [w1_query, value, w2_key],\n",
    "            mask=[query_mask, value_mask],\n",
    "            return_attention_scores = True,\n",
    "        )\n",
    "\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderInput(typing.NamedTuple):\n",
    "    new_tokens: Any\n",
    "    enc_output: Any\n",
    "    mask: Any\n",
    "\n",
    "class DecoderOutput(typing.NamedTuple):\n",
    "    logits: Any\n",
    "    attention_weights: Any\n",
    "\n",
    "\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, output_vocab_size, embedding_dim, dec_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dec_units = dec_units\n",
    "        self.output_vocab_size = output_vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # For Step 1. The embedding layer convets token IDs to vectors\n",
    "        self.embedding = tf.keras.layers.Embedding(self.output_vocab_size,\n",
    "                                                   embedding_dim)\n",
    "\n",
    "        # For Step 2. The RNN keeps track of what's been generated so far.\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "\n",
    "        # For step 3. The RNN output will be the query for the attention layer.\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "        # For step 4. Eqn. (3): converting `ct` to `at`\n",
    "        self.Wc = tf.keras.layers.Dense(dec_units, activation=tf.math.tanh,\n",
    "                                        use_bias=False)\n",
    "\n",
    "        # For step 5. This fully connected layer produces the logits for each\n",
    "        # output token.\n",
    "        self.fc = tf.keras.layers.Dense(self.output_vocab_size)\n",
    "        \n",
    "    def call(self,inputs: DecoderInput,state=None) -> Tuple[DecoderOutput, tf.Tensor]:\n",
    "\n",
    "\n",
    "        # Step 1. Lookup the embeddings\n",
    "        vectors = self.embedding(inputs.new_tokens)\n",
    "\n",
    "        # Step 2. Process one step with the RNN\n",
    "        rnn_output, state = self.gru(vectors, initial_state=state)\n",
    "\n",
    "\n",
    "        # Step 3. Use the RNN output as the query for the attention over the\n",
    "        # encoder output.\n",
    "        context_vector, attention_weights = self.attention(\n",
    "          query=rnn_output, value=inputs.enc_output, mask=inputs.mask)\n",
    "\n",
    "        # Step 4. Eqn. (3): Join the context_vector and rnn_output\n",
    "        #     [ct; ht] shape: (batch t, value_units + query_units)\n",
    "        context_and_rnn_output = tf.concat([context_vector, rnn_output], axis=-1)\n",
    "\n",
    "        # Step 4. Eqn. (3): `at = tanh(Wc@[ct; ht])`\n",
    "        attention_vector = self.Wc(context_and_rnn_output)\n",
    "\n",
    "        # Step 5. Generate logit predictions:\n",
    "        logits = self.fc(attention_vector)\n",
    "\n",
    "        return DecoderOutput(logits, attention_weights), state\n",
    "\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(output_text_processor.vocabulary_size(),\n",
    "                  embedding_dim, units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the target sequence, and collect the \"[START]\" tokens\n",
    "example_output_tokens = output_text_processor(y)\n",
    "\n",
    "start_index = output_text_processor.get_vocabulary().index('[START]')\n",
    "first_token = tf.constant([[start_index]] * example_output_tokens.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits shape: (batch_size, t, output_vocab_size) (64, 1, 5000)\n",
      "state shape: (batch_size, dec_units) (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "dec_result, dec_state = decoder(\n",
    "    inputs = DecoderInput(new_tokens=first_token,\n",
    "                          enc_output=example_enc_output,\n",
    "                          mask=(example_tokens != 0)),\n",
    "    state = example_enc_state\n",
    ")\n",
    "\n",
    "print(f'logits shape: (batch_size, t, output_vocab_size) {dec_result.logits.shape}')\n",
    "print(f'state shape: (batch_size, dec_units) {dec_state.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training del modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self):\n",
    "        self.name = 'masked_loss'\n",
    "        self.loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=True, reduction='none')\n",
    "\n",
    "    def __call__(self, y_true, y_pred):\n",
    "    \n",
    "        # Calculate the loss for each item in the batch.\n",
    "        loss = self.loss(y_true, y_pred)\n",
    "\n",
    "        # Mask off the losses on padding.\n",
    "        mask = tf.cast(y_true != 0, tf.float32)\n",
    "        loss *= mask\n",
    "\n",
    "        # Return the total.\n",
    "        return tf.reduce_sum(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainTranslator(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units,\n",
    "               input_text_processor,\n",
    "               output_text_processor, \n",
    "               use_tf_function=True):\n",
    "        super().__init__()\n",
    "        # Build the encoder and decoder\n",
    "        encoder = Encoder(input_text_processor.vocabulary_size(),\n",
    "                          embedding_dim, units)\n",
    "        decoder = Decoder(output_text_processor.vocabulary_size(),\n",
    "                          embedding_dim, units)\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.input_text_processor = input_text_processor\n",
    "        self.output_text_processor = output_text_processor\n",
    "        self.use_tf_function = use_tf_function\n",
    "\n",
    "    def train_step(self, inputs):\n",
    "        if self.use_tf_function:\n",
    "            return self._tf_train_step(inputs)\n",
    "        else:\n",
    "            return self._train_step(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Finire qua"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}