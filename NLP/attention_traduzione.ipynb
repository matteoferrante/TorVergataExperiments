{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import typing\n",
    "from typing import Any, Tuple\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "import tensorflow_text as tf_text\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"data/ita.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset and separate input from target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    \"\"\"\n",
    "    load the files and return coupled list of input target\n",
    "    \"\"\"\n",
    "    with open(path, \"r\",encoding=\"utf-8\") as f:\n",
    "        text =f.read() \n",
    "\n",
    "    lines = text.splitlines()\n",
    "    pairs = [line.split('\\t') for line in lines]\n",
    "\n",
    "    inp = [inp for targ, inp,attr in pairs]\n",
    "    targ = [targ for targ, inp,attr in pairs]\n",
    "\n",
    "    return targ, inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "targ, inp = load_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(inp)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inp, targ)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we want to include all preprocessing inside the model \n",
    "\n",
    "In order to be able to export it as tf_saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_lower_and_split_punct(text):\n",
    "    # Split accecented characters.\n",
    "    text = tf_text.normalize_utf8(text, 'NFKD')\n",
    "    text = tf.strings.lower(text)\n",
    "    # Keep space, a to z, and select punctuation.\n",
    "    text = tf.strings.regex_replace(text, '[^ a-z.?!,]', '')\n",
    "    # Add spaces around punctuation.\n",
    "    text = tf.strings.regex_replace(text, '[.?!,]', r' \\0 ')\n",
    "    # Strip whitespace.\n",
    "    text = tf.strings.strip(text)\n",
    "\n",
    "    text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = tf.constant('Ciao, tutto bene?')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ciao, tutto bene?\n",
      "[START] ciao ,  tutto bene ? [END]\n"
     ]
    }
   ],
   "source": [
    "print(example_text.numpy().decode())\n",
    "print(tf_lower_and_split_punct(example_text).numpy().decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Vectorization\n",
    "for both input and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_size = 5000\n",
    "\n",
    "input_text_processor = preprocessing.TextVectorization(\n",
    "    standardize=tf_lower_and_split_punct,\n",
    "    max_tokens=max_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', '[START]', '[END]', '.', 'tom', '?', 'non', 'e', 'di']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text_processor.adapt(inp)\n",
    "\n",
    "# Here are the first 10 words from the vocabulary:\n",
    "input_text_processor.get_vocabulary()[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', '[START]', '[END]', '.', 'you', 'tom', 'i', 'to', '?']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_text_processor = preprocessing.TextVectorization(\n",
    "    standardize=tf_lower_and_split_punct,\n",
    "    max_tokens=max_vocab_size)\n",
    "\n",
    "output_text_processor.adapt(targ)\n",
    "output_text_processor.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'Quanto costa quello?' b'Tom era preoccupato per i tuoi figli.'\n",
      " b'Ha preso in affitto un appartamento.' b'Io feci un altro test.'\n",
      " b'Tom mi ha detto che non lo ha mai detto.' b'Ha detto che era bello.'\n",
      " b'La prossima volta sar\\xc3\\xa0 pi\\xc3\\xb9 fortunato.'\n",
      " b\"C'\\xc3\\xa8 molta roba pericolosa in questo magazzino.\"\n",
      " b'A me non piacque il suggerimento di Tom.' b'Tom ha due impieghi.'\n",
      " b'Non lo sapevo che il costo della vita fosse cos\\xc3\\xac alto in Australia.'\n",
      " b'Ha ammesso la sua colpa.'\n",
      " b'Mi \\xc3\\xa8 passato velocemente il raffreddore.'\n",
      " b'Vorrei avere la tua fortuna.' b\"Il secchio \\xc3\\xa8 pieno d'acqua.\"\n",
      " b'Inizialmente non le piaceva il cavallo.'\n",
      " b'Il dipinto si sta deteriorando.'\n",
      " b'Il nostro scaldabagno \\xc3\\xa8 rotto.'\n",
      " b'A me piacciono molti tipi di musica.' b'Tom aveva nostalgia di casa.'\n",
      " b'Potete aiutarmi, per piacere?'\n",
      " b\"L'ufficio sta tenendo una festa di addio.\"\n",
      " b'Non andr\\xc3\\xb2 mai a Boston con te.'\n",
      " b'Io ho messo la mia valigia nel bagagliaio.'\n",
      " b'\\xc3\\x88 diventato un cittadino americano.'\n",
      " b'Penso che troverete questo interessante.' b'Amiamo aiutare.'\n",
      " b'I soldati erano pronti a morire per il loro paese.'\n",
      " b'Quando mio padre venne a casa, io stavo studiando.'\n",
      " b'Questo \\xc3\\xa8 il problema di tutti.'\n",
      " b'Lei ha un marito e due figlie.' b'Tom non \\xc3\\xa8 morto di cancro.'\n",
      " b'Ve ne posso prendere di pi\\xc3\\xb9.' b'A che ora partite?'\n",
      " b\"Lei sa quand'\\xc3\\xa8 il compleanno di Tom?\" b'Vuole dei bambini?'\n",
      " b'Vi aspetter\\xc3\\xa0.' b'Occhio!' b'Tu hai visto Tom?'\n",
      " b'Lei mi mostr\\xc3\\xb2 la sua nuova automobile.'\n",
      " b'Le discussioni sono in corso.'\n",
      " b'Sono stata in ansia per la sicurezza di mio figlio.'\n",
      " b'Preferisco il caff\\xc3\\xa8 al t\\xc3\\xa8 nero.'\n",
      " b'Come vi chiamano le vostre amiche?' b'Non \\xc3\\xa8 vuota.'\n",
      " b'Quanti figli ha?' b\"Chi di voi \\xc3\\xa8 l'avvocato di Tom?\"\n",
      " b'Andr\\xc3\\xb2 a fare acquisti.' b'Tom sembra molto compiaciuto.'\n",
      " b'Non dovevo andare a lavorare oggi.'\n",
      " b\"Io l'ho sempre saputo che sareste tornati.\"\n",
      " b'Tom non pu\\xc3\\xb2 avere gi\\xc3\\xa0 risolto il problema.'\n",
      " b'Sono artisti.' b'Io amai lo spettacolo.' b'Che colore terribile!'\n",
      " b'Quanti dolci dovrei comprare?' b'Rimuovi la benda.'\n",
      " b'Non mi sono incontrata con Tom di recente.'\n",
      " b'Perch\\xc3\\xa9 dovrei farlo?' b\"Ho visitato Boston l'anno scorso.\"\n",
      " b'\\xc3\\x88 un pezzo di torta.' b'Dove ci dovremmo nascondere?'\n",
      " b'Correr\\xc3\\xb2.' b'Che tipo di informazioni state cercando?'], shape=(64,), dtype=string)\n",
      "tf.Tensor(\n",
      "[b'How much is that?' b'Tom was worried about his children.'\n",
      " b'He rented an apartment.' b'I did another test.'\n",
      " b'Tom told me he never said that.' b'She said that he was handsome.'\n",
      " b'Better luck next time.'\n",
      " b\"There's a lot of dangerous stuff in this warehouse.\"\n",
      " b\"I didn't like Tom's suggestion.\" b'Tom has two jobs.'\n",
      " b\"I didn't know that the cost of living was so high in Australia.\"\n",
      " b'He admitted his guilt.' b'I got over my cold quickly.'\n",
      " b'I wish I had your luck.' b'The bucket is full of water.'\n",
      " b\"She didn't like the horse at first.\" b'The painting is deteriorating.'\n",
      " b'Our water heater is broken.' b'I like many kinds of music.'\n",
      " b'Tom was homesick.' b'Can you help me, please?'\n",
      " b'The office is having a farewell party.'\n",
      " b\"I'll never go to Boston with you.\" b'I put my suitcase in the trunk.'\n",
      " b'He became an American citizen.'\n",
      " b\"I think you'll find this interesting.\" b'We love to help.'\n",
      " b'The soldiers were ready to die for their country.'\n",
      " b'When my father came home, I was studying.'\n",
      " b\"This is everyone's problem.\" b'She has a husband and two daughters.'\n",
      " b\"Tom didn't die of cancer.\" b'I can get you more.'\n",
      " b'What time are you leaving?' b\"Do you know when Tom's birthday is?\"\n",
      " b'Do you want kids?' b\"He'll wait for you.\" b'Watch out!'\n",
      " b'Have you seen Tom?' b'She showed me her new car.'\n",
      " b'Discussions are ongoing.' b\"I have been anxious about my son's safety.\"\n",
      " b'I like coffee better than black tea.' b'What do your friends call you?'\n",
      " b\"It's not empty.\" b'How many kids do you have?'\n",
      " b\"Which one of you is Tom's lawyer?\" b\"I'll go shopping.\"\n",
      " b'Tom looks very pleased.' b\"I didn't have to go to work today.\"\n",
      " b\"I always knew you'd be back.\" b\"Tom can't have solved the problem yet.\"\n",
      " b'They are artists.' b'I loved the play.' b'What a dreadful color!'\n",
      " b'How much candy should I buy?' b'Remove the bandage.'\n",
      " b\"I haven't met with Tom recently.\" b'Why would I have to do that?'\n",
      " b'I visited Boston last year.' b\"It's a piece of cake.\"\n",
      " b'Where should we hide?' b\"I'm going to run.\"\n",
      " b'What kind of information are you looking for?'], shape=(64,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for x,y in dataset.take(1):\n",
    "    print(x)\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_tokens = input_text_processor(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'Quanto costa quello?', shape=(), dtype=string)\n",
      "tf.Tensor(\n",
      "[   2  138 1230   75    6    3    0    0    0    0    0    0    0    0\n",
      "    0    0], shape=(16,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(x[0])\n",
    "print(example_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 256\n",
    "units = 1024\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "Pass the tokens, it learns some embeddings of those tokens and then pass them through a GRU RNN to process these vectors. It returns the sequence and the state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, input_vocab_size, embedding_dim, enc_units):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.enc_units = enc_units\n",
    "        self.input_vocab_size = input_vocab_size\n",
    "\n",
    "        # The embedding layer converts tokens to vectors\n",
    "        self.embedding = tf.keras.layers.Embedding(self.input_vocab_size,\n",
    "                                                   embedding_dim)\n",
    "\n",
    "        # The GRU RNN layer processes those vectors sequentially.\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       # Return the sequence and state\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, tokens, state=None):\n",
    "        #shape_checker = ShapeChecker()\n",
    "        #shape_checker(tokens, ('batch', 's'))\n",
    "\n",
    "        # 2. The embedding layer looks up the embedding for each token.\n",
    "        vectors = self.embedding(tokens)\n",
    "        #shape_checker(vectors, ('batch', 's', 'embed_dim'))\n",
    "\n",
    "        # 3. The GRU processes the embedding sequence.\n",
    "        #    output shape: (batch, s, enc_units)\n",
    "        #    state shape: (batch, enc_units)\n",
    "        output, state = self.gru(vectors, initial_state=state)\n",
    "        #shape_checker(output, ('batch', 's', 'enc_units'))\n",
    "        #shape_checker(state, ('batch', 'enc_units'))\n",
    "\n",
    "        # 4. Returns the new sequence and its state.\n",
    "        return output, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch, shape (batch): (64,)\n",
      "Input batch tokens, shape (batch, s): (64, 16)\n",
      "Encoder output, shape (batch, s, units): (64, 16, 1024)\n",
      "Encoder state, shape (batch, units): (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "# Convert the input text to tokens.\n",
    "example_tokens = input_text_processor(x)\n",
    "\n",
    "# Encode the input sequence.\n",
    "encoder = Encoder(input_text_processor.vocabulary_size(),\n",
    "                  embedding_dim, units)\n",
    "example_enc_output, example_enc_state = encoder(example_tokens)\n",
    "\n",
    "print(f'Input batch, shape (batch): {x.shape}')\n",
    "print(f'Input batch tokens, shape (batch, s): {example_tokens.shape}')\n",
    "print(f'Encoder output, shape (batch, s, units): {example_enc_output.shape}')\n",
    "print(f'Encoder state, shape (batch, units): {example_enc_state.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        # For Eqn. (4), the  Bahdanau attention\n",
    "        self.W1 = tf.keras.layers.Dense(units, use_bias=False)\n",
    "        self.W2 = tf.keras.layers.Dense(units, use_bias=False)\n",
    "\n",
    "        self.attention = tf.keras.layers.AdditiveAttention()\n",
    "\n",
    "    def call(self, query, value, mask):\n",
    "    \n",
    "        # From Eqn. (4), `W1@ht`.\n",
    "        w1_query = self.W1(query)\n",
    "\n",
    "        # From Eqn. (4), `W2@hs`.\n",
    "        w2_key = self.W2(value)\n",
    "\n",
    "        query_mask = tf.ones(tf.shape(query)[:-1], dtype=bool)\n",
    "        value_mask = mask\n",
    "\n",
    "        context_vector, attention_weights = self.attention(\n",
    "            inputs = [w1_query, value, w2_key],\n",
    "            mask=[query_mask, value_mask],\n",
    "            return_attention_scores = True,\n",
    "        )\n",
    "\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderInput(typing.NamedTuple):\n",
    "    new_tokens: Any\n",
    "    enc_output: Any\n",
    "    mask: Any\n",
    "\n",
    "class DecoderOutput(typing.NamedTuple):\n",
    "    logits: Any\n",
    "    attention_weights: Any\n",
    "\n",
    "\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, output_vocab_size, embedding_dim, dec_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dec_units = dec_units\n",
    "        self.output_vocab_size = output_vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # For Step 1. The embedding layer convets token IDs to vectors\n",
    "        self.embedding = tf.keras.layers.Embedding(self.output_vocab_size,\n",
    "                                                   embedding_dim)\n",
    "\n",
    "        # For Step 2. The RNN keeps track of what's been generated so far.\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "\n",
    "        # For step 3. The RNN output will be the query for the attention layer.\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "        # For step 4. Eqn. (3): converting `ct` to `at`\n",
    "        self.Wc = tf.keras.layers.Dense(dec_units, activation=tf.math.tanh,\n",
    "                                        use_bias=False)\n",
    "\n",
    "        # For step 5. This fully connected layer produces the logits for each\n",
    "        # output token.\n",
    "        self.fc = tf.keras.layers.Dense(self.output_vocab_size)\n",
    "        \n",
    "    def call(self,inputs: DecoderInput,state=None) -> Tuple[DecoderOutput, tf.Tensor]:\n",
    "\n",
    "\n",
    "        # Step 1. Lookup the embeddings\n",
    "        vectors = self.embedding(inputs.new_tokens)\n",
    "\n",
    "        # Step 2. Process one step with the RNN\n",
    "        rnn_output, state = self.gru(vectors, initial_state=state)\n",
    "\n",
    "\n",
    "        # Step 3. Use the RNN output as the query for the attention over the\n",
    "        # encoder output.\n",
    "        context_vector, attention_weights = self.attention(\n",
    "          query=rnn_output, value=inputs.enc_output, mask=inputs.mask)\n",
    "\n",
    "        # Step 4. Eqn. (3): Join the context_vector and rnn_output\n",
    "        #     [ct; ht] shape: (batch t, value_units + query_units)\n",
    "        context_and_rnn_output = tf.concat([context_vector, rnn_output], axis=-1)\n",
    "\n",
    "        # Step 4. Eqn. (3): `at = tanh(Wc@[ct; ht])`\n",
    "        attention_vector = self.Wc(context_and_rnn_output)\n",
    "\n",
    "        # Step 5. Generate logit predictions:\n",
    "        logits = self.fc(attention_vector)\n",
    "\n",
    "        return DecoderOutput(logits, attention_weights), state\n",
    "\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(output_text_processor.vocabulary_size(),\n",
    "                  embedding_dim, units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the target sequence, and collect the \"[START]\" tokens\n",
    "example_output_tokens = output_text_processor(y)\n",
    "\n",
    "start_index = output_text_processor.get_vocabulary().index('[START]')\n",
    "first_token = tf.constant([[start_index]] * example_output_tokens.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits shape: (batch_size, t, output_vocab_size) (64, 1, 5000)\n",
      "state shape: (batch_size, dec_units) (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "dec_result, dec_state = decoder(\n",
    "    inputs = DecoderInput(new_tokens=first_token,\n",
    "                          enc_output=example_enc_output,\n",
    "                          mask=(example_tokens != 0)),\n",
    "    state = example_enc_state\n",
    ")\n",
    "\n",
    "print(f'logits shape: (batch_size, t, output_vocab_size) {dec_result.logits.shape}')\n",
    "print(f'state shape: (batch_size, dec_units) {dec_state.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training del modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self):\n",
    "        self.name = 'masked_loss'\n",
    "        self.loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=True, reduction='none')\n",
    "\n",
    "    def __call__(self, y_true, y_pred):\n",
    "    \n",
    "        # Calculate the loss for each item in the batch.\n",
    "        loss = self.loss(y_true, y_pred)\n",
    "\n",
    "        # Mask off the losses on padding.\n",
    "        mask = tf.cast(y_true != 0, tf.float32)\n",
    "        loss *= mask\n",
    "\n",
    "        # Return the total.\n",
    "        return tf.reduce_sum(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainTranslator(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units,\n",
    "               input_text_processor,\n",
    "               output_text_processor, \n",
    "               use_tf_function=True):\n",
    "        super().__init__()\n",
    "        # Build the encoder and decoder\n",
    "        encoder = Encoder(input_text_processor.vocabulary_size(),\n",
    "                          embedding_dim, units)\n",
    "        decoder = Decoder(output_text_processor.vocabulary_size(),\n",
    "                          embedding_dim, units)\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.input_text_processor = input_text_processor\n",
    "        self.output_text_processor = output_text_processor\n",
    "        self.use_tf_function = use_tf_function\n",
    "\n",
    "    def train_step(self, inputs):\n",
    "        if self.use_tf_function:\n",
    "            return self._tf_train_step(inputs)\n",
    "        else:\n",
    "            return self._train_step(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _preprocess(self, input_text, target_text):\n",
    "    \n",
    "\n",
    "    # Convert the text to token IDs\n",
    "    input_tokens = self.input_text_processor(input_text)\n",
    "    target_tokens = self.output_text_processor(target_text)\n",
    "\n",
    "\n",
    "    # Convert IDs to masks.\n",
    "    input_mask = input_tokens != 0\n",
    "\n",
    "\n",
    "    target_mask = target_tokens != 0\n",
    "\n",
    "    return input_tokens, input_mask, target_tokens, target_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainTranslator._preprocess = _preprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def _train_step(self, inputs):\n",
    "    input_text, target_text = inputs  \n",
    "\n",
    "    (input_tokens, input_mask,\n",
    "    target_tokens, target_mask) = self._preprocess(input_text, target_text)\n",
    "\n",
    "    max_target_length = tf.shape(target_tokens)[1]\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Encode the input\n",
    "        enc_output, enc_state = self.encoder(input_tokens)\n",
    "\n",
    "\n",
    "        # Initialize the decoder's state to the encoder's final state.\n",
    "        # This only works if the encoder and decoder have the same number of\n",
    "        # units.\n",
    "        dec_state = enc_state\n",
    "        loss = tf.constant(0.0)\n",
    "\n",
    "        for t in tf.range(max_target_length-1):\n",
    "            # Pass in two tokens from the target sequence:\n",
    "            # 1. The current input to the decoder.\n",
    "            # 2. The target for the decoder's next prediction.\n",
    "            new_tokens = target_tokens[:, t:t+2]\n",
    "            step_loss, dec_state = self._loop_step(new_tokens, input_mask,\n",
    "                                             enc_output, dec_state)\n",
    "            loss = loss + step_loss\n",
    "\n",
    "        # Average the loss over all non padding tokens.\n",
    "        average_loss = loss / tf.reduce_sum(tf.cast(target_mask, tf.float32))\n",
    "\n",
    "    # Apply an optimization step\n",
    "    variables = self.trainable_variables \n",
    "    gradients = tape.gradient(average_loss, variables)\n",
    "    self.optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    # Return a dict mapping metric names to current value\n",
    "    return {'batch_loss': average_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainTranslator._train_step = _train_step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _loop_step(self, new_tokens, input_mask, enc_output, dec_state):\n",
    "    input_token, target_token = new_tokens[:, 0:1], new_tokens[:, 1:2]\n",
    "\n",
    "    # Run the decoder one step.\n",
    "    decoder_input = DecoderInput(new_tokens=input_token,\n",
    "                               enc_output=enc_output,\n",
    "                               mask=input_mask)\n",
    "\n",
    "    dec_result, dec_state = self.decoder(decoder_input, state=dec_state)\n",
    "\n",
    "\n",
    "    # `self.loss` returns the total for non-padded tokens\n",
    "    y = target_token\n",
    "    y_pred = dec_result.logits\n",
    "    step_loss = self.loss(y, y_pred)\n",
    "\n",
    "    return step_loss, dec_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainTranslator._loop_step = _loop_step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = TrainTranslator(\n",
    "    embedding_dim, units,\n",
    "    input_text_processor=input_text_processor,\n",
    "    output_text_processor=output_text_processor,\n",
    "    use_tf_function=False)\n",
    "\n",
    "# Configure the loss and optimizer\n",
    "translator.compile(\n",
    "    optimizer=tf.optimizers.Adam(),\n",
    "    loss=MaskedLoss(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchLogs(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "        self.logs = []\n",
    "\n",
    "    def on_train_batch_end(self, n, logs):\n",
    "        self.logs.append(logs[self.key])\n",
    "\n",
    "batch_loss = BatchLogs('batch_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "2147/5501 [==========>...................] - ETA: 28:09 - batch_loss: 1.5664"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "2 root error(s) found.\n  (0) RESOURCE_EXHAUSTED:  OOM when allocating tensor with shape[64,1,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node while/decoder_5/embedding_11/embedding_lookup\n (defined at c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\keras\\layers\\embeddings.py:191)\n]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n\t [[Func/StatefulPartitionedCall/while/body/_59/input_control_node/_1259/_161]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n  (1) RESOURCE_EXHAUSTED:  OOM when allocating tensor with shape[64,1,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node while/decoder_5/embedding_11/embedding_lookup\n (defined at c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\keras\\layers\\embeddings.py:191)\n]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_55323]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node while/decoder_5/embedding_11/embedding_lookup:\nIn[0] while/decoder_5/embedding_11/embedding_lookup/51680:\t\nIn[1] while/strided_slice_1 (defined at C:\\Users\\matte\\AppData\\Local\\Temp/ipykernel_20720/4086220853.py:2)\n\nOperation defined at: (most recent call last)\n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\runpy.py\", line 194, in _run_module_as_main\n>>>     return _run_code(code, main_globals, None,\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\runpy.py\", line 87, in _run_code\n>>>     exec(code, run_globals)\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n>>>     app.launch_new_instance()\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n>>>     app.start()\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n>>>     self.io_loop.start()\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n>>>     self.asyncio_loop.run_forever()\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\asyncio\\base_events.py\", line 570, in run_forever\n>>>     self._run_once()\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\asyncio\\base_events.py\", line 1859, in _run_once\n>>>     handle._run()\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\asyncio\\events.py\", line 81, in _run\n>>>     self._context.run(self._callback, *self._args)\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 457, in dispatch_queue\n>>>     await self.process_one()\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 446, in process_one\n>>>     await dispatch(*args)\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 353, in dispatch_shell\n>>>     await result\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 648, in execute_request\n>>>     reply_content = await reply_content\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 353, in do_execute\n>>>     res = shell.run_cell(code, store_history=store_history, silent=silent)\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n>>>     return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2901, in run_cell\n>>>     result = self._run_cell(\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2947, in _run_cell\n>>>     return runner(coro)\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n>>>     coro.send(None)\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3172, in run_cell_async\n>>>     has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3364, in run_ast_nodes\n>>>     if (await self.run_code(code, result,  async_=asy)):\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3444, in run_code\n>>>     exec(code_obj, self.user_global_ns, self.user_ns)\n>>> \n>>>   File \"C:\\Users\\matte\\AppData\\Local\\Temp/ipykernel_20720/2481970495.py\", line 1, in <module>\n>>>     translator.fit(dataset, epochs=3,\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1216, in fit\n>>>     tmp_logs = self.train_function(iterator)\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\keras\\engine\\training.py\", line 878, in train_function\n>>>     return step_function(self, iterator)\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\keras\\engine\\training.py\", line 867, in step_function\n>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in run_step\n>>>     outputs = model.train_step(data)\n>>> \n>>>   File \"C:\\Users\\matte\\AppData\\Local\\Temp/ipykernel_20720/2301851554.py\", line 23, in train_step\n>>>     return self._train_step(inputs)\n>>> \n>>>   File \"C:\\Users\\matte\\AppData\\Local\\Temp/ipykernel_20720/3028482943.py\", line 21, in _train_step\n>>>     for t in tf.range(max_target_length-1):\n>>> \n>>>   File \"C:\\Users\\matte\\AppData\\Local\\Temp/ipykernel_20720/3028482943.py\", line 26, in _train_step\n>>>     step_loss, dec_state = self._loop_step(new_tokens, input_mask,\n>>> \n>>>   File \"C:\\Users\\matte\\AppData\\Local\\Temp/ipykernel_20720/4086220853.py\", line 9, in _loop_step\n>>>     dec_result, dec_state = self.decoder(decoder_input, state=dec_state)\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1083, in __call__\n>>>     outputs = call_fn(inputs, *args, **kwargs)\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\matte\\AppData\\Local\\Temp/ipykernel_20720/2678724283.py\", line 43, in call\n>>>     vectors = self.embedding(inputs.new_tokens)\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1083, in __call__\n>>>     outputs = call_fn(inputs, *args, **kwargs)\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\keras\\layers\\embeddings.py\", line 191, in call\n>>>     out = tf.nn.embedding_lookup(self.embeddings, inputs)\n>>> \n\nInput Source operations connected to node while/decoder_5/embedding_11/embedding_lookup:\nIn[0] while/decoder_5/embedding_11/embedding_lookup/51680:\t\nIn[1] while/strided_slice_1 (defined at C:\\Users\\matte\\AppData\\Local\\Temp/ipykernel_20720/4086220853.py:2)\n\nOperation defined at: (most recent call last)\n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\runpy.py\", line 194, in _run_module_as_main\n>>>     return _run_code(code, main_globals, None,\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\runpy.py\", line 87, in _run_code\n>>>     exec(code, run_globals)\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n>>>     app.launch_new_instance()\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n>>>     app.start()\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n>>>     self.io_loop.start()\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n>>>     self.asyncio_loop.run_forever()\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\asyncio\\base_events.py\", line 570, in run_forever\n>>>     self._run_once()\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\asyncio\\base_events.py\", line 1859, in _run_once\n>>>     handle._run()\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\asyncio\\events.py\", line 81, in _run\n>>>     self._context.run(self._callback, *self._args)\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 457, in dispatch_queue\n>>>     await self.process_one()\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 446, in process_one\n>>>     await dispatch(*args)\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 353, in dispatch_shell\n>>>     await result\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 648, in execute_request\n>>>     reply_content = await reply_content\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 353, in do_execute\n>>>     res = shell.run_cell(code, store_history=store_history, silent=silent)\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n>>>     return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2901, in run_cell\n>>>     result = self._run_cell(\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2947, in _run_cell\n>>>     return runner(coro)\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n>>>     coro.send(None)\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3172, in run_cell_async\n>>>     has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3364, in run_ast_nodes\n>>>     if (await self.run_code(code, result,  async_=asy)):\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3444, in run_code\n>>>     exec(code_obj, self.user_global_ns, self.user_ns)\n>>> \n>>>   File \"C:\\Users\\matte\\AppData\\Local\\Temp/ipykernel_20720/2481970495.py\", line 1, in <module>\n>>>     translator.fit(dataset, epochs=3,\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1216, in fit\n>>>     tmp_logs = self.train_function(iterator)\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\keras\\engine\\training.py\", line 878, in train_function\n>>>     return step_function(self, iterator)\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\keras\\engine\\training.py\", line 867, in step_function\n>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in run_step\n>>>     outputs = model.train_step(data)\n>>> \n>>>   File \"C:\\Users\\matte\\AppData\\Local\\Temp/ipykernel_20720/2301851554.py\", line 23, in train_step\n>>>     return self._train_step(inputs)\n>>> \n>>>   File \"C:\\Users\\matte\\AppData\\Local\\Temp/ipykernel_20720/3028482943.py\", line 21, in _train_step\n>>>     for t in tf.range(max_target_length-1):\n>>> \n>>>   File \"C:\\Users\\matte\\AppData\\Local\\Temp/ipykernel_20720/3028482943.py\", line 26, in _train_step\n>>>     step_loss, dec_state = self._loop_step(new_tokens, input_mask,\n>>> \n>>>   File \"C:\\Users\\matte\\AppData\\Local\\Temp/ipykernel_20720/4086220853.py\", line 9, in _loop_step\n>>>     dec_result, dec_state = self.decoder(decoder_input, state=dec_state)\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1083, in __call__\n>>>     outputs = call_fn(inputs, *args, **kwargs)\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\matte\\AppData\\Local\\Temp/ipykernel_20720/2678724283.py\", line 43, in call\n>>>     vectors = self.embedding(inputs.new_tokens)\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1083, in __call__\n>>>     outputs = call_fn(inputs, *args, **kwargs)\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\keras\\layers\\embeddings.py\", line 191, in call\n>>>     out = tf.nn.embedding_lookup(self.embeddings, inputs)\n>>> \n\nFunction call stack:\ntrain_function -> while_body_51644_rewritten -> train_function -> while_body_51644_rewritten\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20720/2481970495.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m translator.fit(dataset, epochs=3,\n\u001b[0m\u001b[0;32m      2\u001b[0m                      callbacks=[batch_loss])\n",
      "\u001b[1;32mc:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     59\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     60\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: 2 root error(s) found.\n  (0) RESOURCE_EXHAUSTED:  OOM when allocating tensor with shape[64,1,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node while/decoder_5/embedding_11/embedding_lookup\n (defined at c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\keras\\layers\\embeddings.py:191)\n]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n\t [[Func/StatefulPartitionedCall/while/body/_59/input_control_node/_1259/_161]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n  (1) RESOURCE_EXHAUSTED:  OOM when allocating tensor with shape[64,1,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node while/decoder_5/embedding_11/embedding_lookup\n (defined at c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\keras\\layers\\embeddings.py:191)\n]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_55323]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node while/decoder_5/embedding_11/embedding_lookup:\nIn[0] while/decoder_5/embedding_11/embedding_lookup/51680:\t\nIn[1] while/strided_slice_1 (defined at C:\\Users\\matte\\AppData\\Local\\Temp/ipykernel_20720/4086220853.py:2)\n\nOperation defined at: (most recent call last)\n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\runpy.py\", line 194, in _run_module_as_main\n>>>     return _run_code(code, main_globals, None,\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\runpy.py\", line 87, in _run_code\n>>>     exec(code, run_globals)\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n>>>     app.launch_new_instance()\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n>>>     app.start()\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n>>>     self.io_loop.start()\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n>>>     self.asyncio_loop.run_forever()\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\asyncio\\base_events.py\", line 570, in run_forever\n>>>     self._run_once()\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\asyncio\\base_events.py\", line 1859, in _run_once\n>>>     handle._run()\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\asyncio\\events.py\", line 81, in _run\n>>>     self._context.run(self._callback, *self._args)\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 457, in dispatch_queue\n>>>     await self.process_one()\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 446, in process_one\n>>>     await dispatch(*args)\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 353, in dispatch_shell\n>>>     await result\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 648, in execute_request\n>>>     reply_content = await reply_content\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 353, in do_execute\n>>>     res = shell.run_cell(code, store_history=store_history, silent=silent)\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n>>>     return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2901, in run_cell\n>>>     result = self._run_cell(\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2947, in _run_cell\n>>>     return runner(coro)\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n>>>     coro.send(None)\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3172, in run_cell_async\n>>>     has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3364, in run_ast_nodes\n>>>     if (await self.run_code(code, result,  async_=asy)):\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3444, in run_code\n>>>     exec(code_obj, self.user_global_ns, self.user_ns)\n>>> \n>>>   File \"C:\\Users\\matte\\AppData\\Local\\Temp/ipykernel_20720/2481970495.py\", line 1, in <module>\n>>>     translator.fit(dataset, epochs=3,\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1216, in fit\n>>>     tmp_logs = self.train_function(iterator)\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\keras\\engine\\training.py\", line 878, in train_function\n>>>     return step_function(self, iterator)\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\keras\\engine\\training.py\", line 867, in step_function\n>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in run_step\n>>>     outputs = model.train_step(data)\n>>> \n>>>   File \"C:\\Users\\matte\\AppData\\Local\\Temp/ipykernel_20720/2301851554.py\", line 23, in train_step\n>>>     return self._train_step(inputs)\n>>> \n>>>   File \"C:\\Users\\matte\\AppData\\Local\\Temp/ipykernel_20720/3028482943.py\", line 21, in _train_step\n>>>     for t in tf.range(max_target_length-1):\n>>> \n>>>   File \"C:\\Users\\matte\\AppData\\Local\\Temp/ipykernel_20720/3028482943.py\", line 26, in _train_step\n>>>     step_loss, dec_state = self._loop_step(new_tokens, input_mask,\n>>> \n>>>   File \"C:\\Users\\matte\\AppData\\Local\\Temp/ipykernel_20720/4086220853.py\", line 9, in _loop_step\n>>>     dec_result, dec_state = self.decoder(decoder_input, state=dec_state)\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1083, in __call__\n>>>     outputs = call_fn(inputs, *args, **kwargs)\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\matte\\AppData\\Local\\Temp/ipykernel_20720/2678724283.py\", line 43, in call\n>>>     vectors = self.embedding(inputs.new_tokens)\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1083, in __call__\n>>>     outputs = call_fn(inputs, *args, **kwargs)\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\keras\\layers\\embeddings.py\", line 191, in call\n>>>     out = tf.nn.embedding_lookup(self.embeddings, inputs)\n>>> \n\nInput Source operations connected to node while/decoder_5/embedding_11/embedding_lookup:\nIn[0] while/decoder_5/embedding_11/embedding_lookup/51680:\t\nIn[1] while/strided_slice_1 (defined at C:\\Users\\matte\\AppData\\Local\\Temp/ipykernel_20720/4086220853.py:2)\n\nOperation defined at: (most recent call last)\n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\runpy.py\", line 194, in _run_module_as_main\n>>>     return _run_code(code, main_globals, None,\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\runpy.py\", line 87, in _run_code\n>>>     exec(code, run_globals)\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n>>>     app.launch_new_instance()\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n>>>     app.start()\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n>>>     self.io_loop.start()\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n>>>     self.asyncio_loop.run_forever()\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\asyncio\\base_events.py\", line 570, in run_forever\n>>>     self._run_once()\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\asyncio\\base_events.py\", line 1859, in _run_once\n>>>     handle._run()\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\asyncio\\events.py\", line 81, in _run\n>>>     self._context.run(self._callback, *self._args)\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 457, in dispatch_queue\n>>>     await self.process_one()\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 446, in process_one\n>>>     await dispatch(*args)\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 353, in dispatch_shell\n>>>     await result\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 648, in execute_request\n>>>     reply_content = await reply_content\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 353, in do_execute\n>>>     res = shell.run_cell(code, store_history=store_history, silent=silent)\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n>>>     return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2901, in run_cell\n>>>     result = self._run_cell(\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2947, in _run_cell\n>>>     return runner(coro)\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n>>>     coro.send(None)\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3172, in run_cell_async\n>>>     has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3364, in run_ast_nodes\n>>>     if (await self.run_code(code, result,  async_=asy)):\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3444, in run_code\n>>>     exec(code_obj, self.user_global_ns, self.user_ns)\n>>> \n>>>   File \"C:\\Users\\matte\\AppData\\Local\\Temp/ipykernel_20720/2481970495.py\", line 1, in <module>\n>>>     translator.fit(dataset, epochs=3,\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1216, in fit\n>>>     tmp_logs = self.train_function(iterator)\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\keras\\engine\\training.py\", line 878, in train_function\n>>>     return step_function(self, iterator)\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\keras\\engine\\training.py\", line 867, in step_function\n>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in run_step\n>>>     outputs = model.train_step(data)\n>>> \n>>>   File \"C:\\Users\\matte\\AppData\\Local\\Temp/ipykernel_20720/2301851554.py\", line 23, in train_step\n>>>     return self._train_step(inputs)\n>>> \n>>>   File \"C:\\Users\\matte\\AppData\\Local\\Temp/ipykernel_20720/3028482943.py\", line 21, in _train_step\n>>>     for t in tf.range(max_target_length-1):\n>>> \n>>>   File \"C:\\Users\\matte\\AppData\\Local\\Temp/ipykernel_20720/3028482943.py\", line 26, in _train_step\n>>>     step_loss, dec_state = self._loop_step(new_tokens, input_mask,\n>>> \n>>>   File \"C:\\Users\\matte\\AppData\\Local\\Temp/ipykernel_20720/4086220853.py\", line 9, in _loop_step\n>>>     dec_result, dec_state = self.decoder(decoder_input, state=dec_state)\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1083, in __call__\n>>>     outputs = call_fn(inputs, *args, **kwargs)\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\matte\\AppData\\Local\\Temp/ipykernel_20720/2678724283.py\", line 43, in call\n>>>     vectors = self.embedding(inputs.new_tokens)\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1083, in __call__\n>>>     outputs = call_fn(inputs, *args, **kwargs)\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"c:\\users\\matte\\anaconda3\\envs\\ai_env\\lib\\site-packages\\keras\\layers\\embeddings.py\", line 191, in call\n>>>     out = tf.nn.embedding_lookup(self.embeddings, inputs)\n>>> \n\nFunction call stack:\ntrain_function -> while_body_51644_rewritten -> train_function -> while_body_51644_rewritten\n"
     ]
    }
   ],
   "source": [
    "translator.fit(dataset, epochs=3,\n",
    "                     callbacks=[batch_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator(tf.Module):\n",
    "\n",
    "    def __init__(self, encoder, decoder, input_text_processor,\n",
    "               output_text_processor):\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.input_text_processor = input_text_processor\n",
    "        self.output_text_processor = output_text_processor\n",
    "\n",
    "        self.output_token_string_from_index = (\n",
    "            tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "                vocabulary=output_text_processor.get_vocabulary(),\n",
    "                mask_token='',\n",
    "                invert=True))\n",
    "\n",
    "        # The output should never generate padding, unknown, or start.\n",
    "        index_from_string = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "            vocabulary=output_text_processor.get_vocabulary(), mask_token='')\n",
    "        token_mask_ids = index_from_string(['', '[UNK]', '[START]']).numpy()\n",
    "\n",
    "        token_mask = np.zeros([index_from_string.vocabulary_size()], dtype=np.bool)\n",
    "        token_mask[np.array(token_mask_ids)] = True\n",
    "        self.token_mask = token_mask\n",
    "\n",
    "        self.start_token = index_from_string(tf.constant('[START]'))\n",
    "        self.end_token = index_from_string(tf.constant('[END]'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator(\n",
    "    encoder=translator.encoder,\n",
    "    decoder=translator.decoder,\n",
    "    input_text_processor=input_text_processor,\n",
    "    output_text_processor=output_text_processor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_text(self, result_tokens):\n",
    "\n",
    "    result_text_tokens = self.output_token_string_from_index(result_tokens)\n",
    "\n",
    "    result_text = tf.strings.reduce_join(result_text_tokens,\n",
    "                                       axis=1, separator=' ')\n",
    "\n",
    "    result_text = tf.strings.strip(result_text)\n",
    "    return result_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "Translator.tokens_to_text = tokens_to_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'insisted name', b'where wounded', b'pompous unacceptable',\n",
       "       b'audience beautiful', b'happily survivors'], dtype=object)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_output_tokens = tf.random.uniform(\n",
    "    shape=[5, 2], minval=0, dtype=tf.int64,\n",
    "    maxval=output_text_processor.vocabulary_size())\n",
    "translator.tokens_to_text(example_output_tokens).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(self, logits, temperature):\n",
    "\n",
    "    token_mask = self.token_mask[tf.newaxis, tf.newaxis, :]\n",
    "  \n",
    "\n",
    "    # Set the logits for all masked tokens to -inf, so they are never chosen.\n",
    "    logits = tf.where(self.token_mask, -np.inf, logits)\n",
    "\n",
    "    if temperature == 0.0:\n",
    "        new_tokens = tf.argmax(logits, axis=-1)\n",
    "    else: \n",
    "        logits = tf.squeeze(logits, axis=1)\n",
    "        new_tokens = tf.random.categorical(logits/temperature,\n",
    "                                            num_samples=1)\n",
    "\n",
    "\n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "Translator.sample = sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_unrolled(self,\n",
    "                       input_text, *,\n",
    "                       max_length=50,\n",
    "                       return_attention=True,\n",
    "                       temperature=1.0):\n",
    "    \n",
    "    batch_size = tf.shape(input_text)[0]\n",
    "    input_tokens = self.input_text_processor(input_text)\n",
    "    enc_output, enc_state = self.encoder(input_tokens)\n",
    "\n",
    "    dec_state = enc_state\n",
    "    new_tokens = tf.fill([batch_size, 1], self.start_token)\n",
    "\n",
    "    result_tokens = []\n",
    "    attention = []\n",
    "    done = tf.zeros([batch_size, 1], dtype=tf.bool)\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        dec_input = DecoderInput(new_tokens=new_tokens,\n",
    "                                 enc_output=enc_output,\n",
    "                                 mask=(input_tokens!=0))\n",
    "\n",
    "        dec_result, dec_state = self.decoder(dec_input, state=dec_state)\n",
    "\n",
    "        attention.append(dec_result.attention_weights)\n",
    "\n",
    "        new_tokens = self.sample(dec_result.logits, temperature)\n",
    "\n",
    "        # If a sequence produces an `end_token`, set it `done`\n",
    "        done = done | (new_tokens == self.end_token)\n",
    "        # Once a sequence is done it only produces 0-padding.\n",
    "        new_tokens = tf.where(done, tf.constant(0, dtype=tf.int64), new_tokens)\n",
    "\n",
    "        # Collect the generated tokens\n",
    "        result_tokens.append(new_tokens)\n",
    "\n",
    "        if tf.executing_eagerly() and tf.reduce_all(done):\n",
    "            break\n",
    "\n",
    "    # Convert the list of generates token ids to a list of strings.\n",
    "    result_tokens = tf.concat(result_tokens, axis=-1)\n",
    "    result_text = self.tokens_to_text(result_tokens)\n",
    "\n",
    "    if return_attention:\n",
    "        attention_stack = tf.concat(attention, axis=1)\n",
    "        return {'text': result_text, 'attention': attention_stack}\n",
    "    else:\n",
    "        return {'text': result_text}\n",
    "\n",
    "Translator.translate = translate_unrolled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it took a very cold here .\n",
      "they figured all of everyone .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_text = tf.constant([\n",
    "    'Fa molto freddo qua.', # \"It's really cold here.\"\n",
    "    'Buongiorno a tutti.', # \"This is my life.\"\"\n",
    "])\n",
    "\n",
    "result = translator.translate(\n",
    "    input_text = input_text)\n",
    "\n",
    "print(result['text'][0].numpy().decode())\n",
    "print(result['text'][1].numpy().decode())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
